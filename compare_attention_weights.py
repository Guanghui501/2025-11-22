#!/usr/bin/env python
"""
å¯¹æ¯”å…¨æ¨¡æ€å’Œæ— ä¸­æœŸèåˆæ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡

åˆ†æä¸­æœŸèåˆå¦‚ä½•æ”¹å–„èŠ‚ç‚¹-æ–‡æœ¬å¯¹é½
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import entropy
import pandas as pd
from tqdm import tqdm
import argparse

# æ·»åŠ è·¯å¾„
import sys
sys.path.insert(0, os.path.dirname(__file__))

from models.alignn import ALIGNN
from data import get_train_val_loaders


class AttentionComparator:
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡"""

    def __init__(self, checkpoint_full, checkpoint_no_middle, root_dir, device='cuda'):
        """
        Args:
            checkpoint_full: å…¨æ¨¡æ€æ¨¡å‹ï¼ˆä¸­æœŸ+ç»†ç²’åº¦+å…¨å±€ï¼‰
            checkpoint_no_middle: æ— ä¸­æœŸèåˆæ¨¡å‹ï¼ˆç»†ç²’åº¦+å…¨å±€ï¼‰
            root_dir: æ•°æ®ç›®å½•
        """
        self.device = device
        self.root_dir = root_dir

        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹...")
        print(f"  å…¨æ¨¡æ€: {checkpoint_full}")
        self.model_full = self._load_model(checkpoint_full)

        print(f"  æ— ä¸­æœŸ: {checkpoint_no_middle}")
        self.model_no_middle = self._load_model(checkpoint_no_middle)

        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

    def _load_model(self, checkpoint_path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

        # æ‰“å° checkpoint çš„é”®ï¼Œæ–¹ä¾¿è°ƒè¯•
        print(f"    Checkpoint keys: {list(checkpoint.keys())}")

        # è·å–é…ç½®
        if 'config' in checkpoint:
            config = checkpoint['config']
        elif 'model_config' in checkpoint:
            config = checkpoint['model_config']
        else:
            raise KeyError(f"Cannot find config in checkpoint. Available keys: {list(checkpoint.keys())}")

        # é‡å»ºæ¨¡å‹
        model = ALIGNN(config)

        # å°è¯•å¤šç§å¯èƒ½çš„çŠ¶æ€å­—å…¸é”®å
        state_dict = None
        possible_keys = ['model_state_dict', 'state_dict', 'model', 'model_state']

        for key in possible_keys:
            if key in checkpoint:
                state_dict = checkpoint[key]
                print(f"    Found state dict with key: '{key}'")
                break

        if state_dict is None:
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ checkpointï¼ˆå¯èƒ½æ•´ä¸ªæ–‡ä»¶å°±æ˜¯ state_dictï¼‰
            if all(isinstance(k, str) and not k.startswith('_') for k in checkpoint.keys()):
                state_dict = checkpoint
                print(f"    Using entire checkpoint as state dict")
            else:
                raise KeyError(f"Cannot find model state dict. Available keys: {list(checkpoint.keys())}")

        model.load_state_dict(state_dict)
        model.to(self.device)
        model.eval()

        return model

    def extract_attention_weights(self, dataloader, num_samples=100):
        """
        ä»ä¸¤ä¸ªæ¨¡å‹æå–æ³¨æ„åŠ›æƒé‡

        Returns:
            dict: {
                'full_model': {
                    'fine_grained': [...],  # ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡
                    'cross_modal': [...]     # å…¨å±€æ³¨æ„åŠ›æƒé‡
                },
                'no_middle_model': {...}
            }
        """
        results = {
            'full_model': {
                'fine_grained': [],
                'cross_modal': [],
                'sample_ids': []
            },
            'no_middle_model': {
                'fine_grained': [],
                'cross_modal': [],
                'sample_ids': []
            }
        }

        print(f"æå–æ³¨æ„åŠ›æƒé‡ï¼ˆå‰ {num_samples} ä¸ªæ ·æœ¬ï¼‰...")

        count = 0
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Processing"):
                if count >= num_samples:
                    break

                g, lg, text, target = batch
                g = g.to(self.device)
                if lg is not None:
                    lg = lg.to(self.device)

                batch_size = target.size(0)

                # æå–å…¨æ¨¡æ€æ¨¡å‹çš„æ³¨æ„åŠ›
                model_input = (g, lg, text) if lg is not None else (g, text)
                output_full = self.model_full(model_input, return_attention=True)

                # æå–æ— ä¸­æœŸæ¨¡å‹çš„æ³¨æ„åŠ›
                output_no_middle = self.model_no_middle(model_input, return_attention=True)

                # ä¿å­˜æ³¨æ„åŠ›æƒé‡
                # ç»†ç²’åº¦æ³¨æ„åŠ› (fine_grained_attention_weights æ˜¯ä¸€ä¸ªå­—å…¸)
                if 'fine_grained_attention_weights' in output_full:
                    # ä½¿ç”¨ atom_to_text æ³¨æ„åŠ›: [batch, heads, num_atoms, seq_len]
                    # è¿™æ˜¾ç¤ºæ¯ä¸ªåŸå­å…³æ³¨å“ªäº›æ–‡æœ¬token
                    attn_full = output_full['fine_grained_attention_weights']['atom_to_text']
                    attn_no_middle = output_no_middle['fine_grained_attention_weights']['atom_to_text']

                    # å¯¹å¤šå¤´æ³¨æ„åŠ›å–å¹³å‡: [batch, heads, num_atoms, seq_len] -> [batch, num_atoms, seq_len]
                    attn_full = attn_full.mean(dim=1)  # å¹³å‡æ‰€æœ‰æ³¨æ„åŠ›å¤´
                    attn_no_middle = attn_no_middle.mean(dim=1)

                    results['full_model']['fine_grained'].append(
                        attn_full.cpu().numpy()
                    )
                    results['no_middle_model']['fine_grained'].append(
                        attn_no_middle.cpu().numpy()
                    )

                # å…¨å±€æ³¨æ„åŠ› (cross-modal attentionï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå­—å…¸)
                if 'attention_weights' in output_full:
                    # ä½¿ç”¨ graph_to_text æ³¨æ„åŠ›: [batch, heads, 1, 1]
                    attn_full = output_full['attention_weights']['graph_to_text']
                    attn_no_middle = output_no_middle['attention_weights']['graph_to_text']

                    # å¯¹å¤šå¤´æ³¨æ„åŠ›å–å¹³å‡: [batch, heads, 1, 1] -> [batch, 1, 1]
                    attn_full = attn_full.mean(dim=1)
                    attn_no_middle = attn_no_middle.mean(dim=1)

                    results['full_model']['cross_modal'].append(
                        attn_full.cpu().numpy()
                    )
                    results['no_middle_model']['cross_modal'].append(
                        attn_no_middle.cpu().numpy()
                    )

                # è®°å½•æ ·æœ¬IDï¼ˆåªè¦æœ‰ä»»ä½•æ³¨æ„åŠ›æƒé‡ï¼‰
                if 'fine_grained_attention_weights' in output_full or 'attention_weights' in output_full:
                    results['full_model']['sample_ids'].extend(range(count, count + batch_size))
                    results['no_middle_model']['sample_ids'].extend(range(count, count + batch_size))

                count += batch_size

        print(f"âœ… æå–å®Œæˆï¼š{count} ä¸ªæ ·æœ¬")
        print(f"  ç»†ç²’åº¦æ³¨æ„åŠ›: {len(results['full_model']['fine_grained'])} æ‰¹æ¬¡")
        print(f"  å…¨å±€æ³¨æ„åŠ›: {len(results['full_model']['cross_modal'])} æ‰¹æ¬¡\n")

        return results

    def compute_attention_statistics(self, attention_weights):
        """
        è®¡ç®—æ³¨æ„åŠ›æƒé‡çš„ç»Ÿè®¡æŒ‡æ ‡

        Returns:
            dict: å„ç§ç»Ÿè®¡æŒ‡æ ‡
        """
        stats = {
            'entropy': [],           # ç†µï¼ˆåˆ†å¸ƒé›†ä¸­åº¦ï¼‰
            'max_weight': [],        # æœ€å¤§æƒé‡
            'effective_tokens': [],  # æœ‰æ•ˆtokenæ•°ï¼ˆæƒé‡>0.1ï¼‰
            'gini': []              # åŸºå°¼ç³»æ•°ï¼ˆä¸å¹³ç­‰åº¦ï¼‰
        }

        for attn in attention_weights:
            # attn shape: [batch, num_atoms, num_tokens]
            for sample_attn in attn:
                # å¯¹æ¯ä¸ªåŸå­çš„æ³¨æ„åŠ›åˆ†å¸ƒ
                for atom_attn in sample_attn:
                    # å½’ä¸€åŒ–
                    atom_attn = atom_attn / (atom_attn.sum() + 1e-8)

                    # ç†µï¼ˆè¶Šä½è¶Šé›†ä¸­ï¼‰
                    stats['entropy'].append(entropy(atom_attn + 1e-8))

                    # æœ€å¤§æƒé‡
                    stats['max_weight'].append(atom_attn.max())

                    # æœ‰æ•ˆtokenæ•°
                    stats['effective_tokens'].append((atom_attn > 0.1).sum())

                    # åŸºå°¼ç³»æ•°
                    sorted_attn = np.sort(atom_attn)
                    n = len(sorted_attn)
                    gini = (2 * np.sum((np.arange(n) + 1) * sorted_attn)) / (n * np.sum(sorted_attn)) - (n + 1) / n
                    stats['gini'].append(gini)

        # è®¡ç®—å¹³å‡å€¼
        return {k: np.mean(v) for k, v in stats.items()}

    def visualize_attention_comparison(self, results, save_dir):
        """å¯è§†åŒ–æ³¨æ„åŠ›å¯¹æ¯”"""
        os.makedirs(save_dir, exist_ok=True)

        print("ç”Ÿæˆå¯è§†åŒ–...")

        # 1. ç»†ç²’åº¦æ³¨æ„åŠ›ç»Ÿè®¡å¯¹æ¯”
        if not results['full_model']['fine_grained']:
            print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡ï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–")
            print("   è¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨äº† use_fine_grained_attention")
            return

        if results['full_model']['fine_grained']:
            stats_full = self.compute_attention_statistics(
                results['full_model']['fine_grained']
            )
            stats_no_middle = self.compute_attention_statistics(
                results['no_middle_model']['fine_grained']
            )

            # ç»˜åˆ¶å¯¹æ¯”å›¾
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle('ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å¯¹æ¯”ï¼šå…¨æ¨¡æ€ vs æ— ä¸­æœŸèåˆ', fontsize=14, weight='bold')

            metrics = ['entropy', 'max_weight', 'effective_tokens', 'gini']
            titles = [
                'æ³¨æ„åŠ›ç†µï¼ˆè¶Šä½è¶Šé›†ä¸­ï¼‰',
                'æœ€å¤§æ³¨æ„åŠ›æƒé‡ï¼ˆè¶Šé«˜è¶Šæ˜ç¡®ï¼‰',
                'æœ‰æ•ˆTokenæ•°ï¼ˆè¶Šå°‘è¶Šé€‰æ‹©æ€§ï¼‰',
                'åŸºå°¼ç³»æ•°ï¼ˆè¶Šé«˜è¶Šä¸å¹³ç­‰ï¼‰'
            ]

            for idx, (metric, title) in enumerate(zip(metrics, titles)):
                ax = axes[idx // 2, idx % 2]

                values = [stats_full[metric], stats_no_middle[metric]]
                labels = ['å…¨æ¨¡æ€\n(æœ‰ä¸­æœŸèåˆ)', 'æ— ä¸­æœŸèåˆ']
                colors = ['#2ecc71', '#e74c3c']

                bars = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor='black')
                ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)
                ax.set_title(title, fontsize=12, weight='bold')
                ax.grid(axis='y', alpha=0.3)

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, val in zip(bars, values):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{val:.4f}',
                           ha='center', va='bottom', fontsize=10, weight='bold')

                # æ·»åŠ æ”¹å–„ç™¾åˆ†æ¯”
                if stats_full[metric] != 0:
                    improvement = (stats_full[metric] - stats_no_middle[metric]) / abs(stats_no_middle[metric]) * 100
                    ax.text(0.5, 0.95, f'æ”¹å–„: {improvement:+.1f}%',
                           transform=ax.transAxes,
                           ha='center', va='top',
                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                           fontsize=9)

            plt.tight_layout()
            plt.savefig(f'{save_dir}/attention_statistics_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
            print(f"  âœ… ä¿å­˜: attention_statistics_comparison.png")

            # 2. ç»˜åˆ¶ç¤ºä¾‹æ³¨æ„åŠ›çƒ­å›¾
            self._plot_attention_heatmaps(results, save_dir)

            # 3. ä¿å­˜ç»Ÿè®¡æ•°æ®
            comparison_df = pd.DataFrame({
                'Metric': metrics,
                'Full Model (with Middle Fusion)': [stats_full[m] for m in metrics],
                'No Middle Fusion': [stats_no_middle[m] for m in metrics],
                'Improvement (%)': [
                    (stats_full[m] - stats_no_middle[m]) / abs(stats_no_middle[m]) * 100
                    for m in metrics
                ]
            })
            comparison_df.to_csv(f'{save_dir}/attention_statistics.csv', index=False)
            print(f"  âœ… ä¿å­˜: attention_statistics.csv")

            # æ‰“å°ç»“æœ
            print("\n" + "="*80)
            print("ğŸ“Š ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡å¯¹æ¯”")
            print("="*80)
            print(comparison_df.to_string(index=False))
            print("="*80 + "\n")

    def _plot_attention_heatmaps(self, results, save_dir, num_examples=3):
        """ç»˜åˆ¶æ³¨æ„åŠ›çƒ­å›¾ç¤ºä¾‹"""

        fine_grained_full = results['full_model']['fine_grained']
        fine_grained_no_middle = results['no_middle_model']['fine_grained']

        if not fine_grained_full:
            return

        # é€‰æ‹©å‡ ä¸ªç¤ºä¾‹
        for idx in range(min(num_examples, len(fine_grained_full))):
            attn_full = fine_grained_full[idx][0]  # ç¬¬ä¸€ä¸ªæ ·æœ¬
            attn_no_middle = fine_grained_no_middle[idx][0]

            fig, axes = plt.subplots(1, 2, figsize=(14, 5))
            fig.suptitle(f'ç¤ºä¾‹ {idx+1}: èŠ‚ç‚¹-Token æ³¨æ„åŠ›çƒ­å›¾å¯¹æ¯”', fontsize=14, weight='bold')

            # å…¨æ¨¡æ€
            sns.heatmap(attn_full, cmap='YlOrRd', ax=axes[0],
                       cbar_kws={'label': 'Attention Weight'})
            axes[0].set_title('å…¨æ¨¡æ€ï¼ˆæœ‰ä¸­æœŸèåˆï¼‰', fontsize=12)
            axes[0].set_xlabel('Text Tokens', fontsize=11)
            axes[0].set_ylabel('Graph Nodes', fontsize=11)

            # æ— ä¸­æœŸ
            sns.heatmap(attn_no_middle, cmap='YlOrRd', ax=axes[1],
                       cbar_kws={'label': 'Attention Weight'})
            axes[1].set_title('æ— ä¸­æœŸèåˆ', fontsize=12)
            axes[1].set_xlabel('Text Tokens', fontsize=11)
            axes[1].set_ylabel('Graph Nodes', fontsize=11)

            plt.tight_layout()
            plt.savefig(f'{save_dir}/attention_heatmap_example_{idx+1}.png', dpi=300, bbox_inches='tight')
            plt.close()

        print(f"  âœ… ä¿å­˜: {num_examples} ä¸ªæ³¨æ„åŠ›çƒ­å›¾ç¤ºä¾‹")


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”æ³¨æ„åŠ›æƒé‡')
    parser.add_argument('--checkpoint_full', type=str, required=True,
                       help='å…¨æ¨¡æ€æ¨¡å‹checkpoint')
    parser.add_argument('--checkpoint_no_middle', type=str, required=True,
                       help='æ— ä¸­æœŸèåˆæ¨¡å‹checkpoint')
    parser.add_argument('--root_dir', type=str, required=True,
                       help='æ•°æ®ç›®å½•')
    parser.add_argument('--dataset', type=str, default='dft_3d',
                       help='æ•°æ®é›†ç±»å‹ï¼ˆå¦‚ dft_3d, mp ç­‰ï¼‰')
    parser.add_argument('--property', type=str, default='mbj_bandgap',
                       help='å±æ€§åç§°ï¼ˆå¦‚ mbj_bandgap, formation_energy ç­‰ï¼‰')
    parser.add_argument('--save_dir', type=str, default='./attention_comparison',
                       help='ä¿å­˜ç›®å½•')
    parser.add_argument('--num_samples', type=int, default=100,
                       help='åˆ†æçš„æ ·æœ¬æ•°')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹å¤§å°')

    args = parser.parse_args()

    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = AttentionComparator(
        args.checkpoint_full,
        args.checkpoint_no_middle,
        args.root_dir
    )

    # åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨ä¸ compare_fusion_mechanisms.py ç›¸åŒçš„æ–¹æ³•ï¼‰
    print(f"\nğŸ”„ åŠ è½½æ•°æ®é›†: {args.dataset} - {args.property}")
    try:
        from train_with_cross_modal_attention import load_dataset, get_dataset_paths

        # è·å–æ•°æ®é›†è·¯å¾„
        cif_dir, id_prop_file = get_dataset_paths(args.root_dir, args.dataset, args.property)
        print(f"  æ•°æ®æ–‡ä»¶: {id_prop_file}")
        print(f"  ç»“æ„ç›®å½•: {cif_dir}")

        # åŠ è½½æ•°æ®é›†
        df = load_dataset(cif_dir, id_prop_file, args.dataset, args.property)
        print(f"âœ…  åŠ è½½æ•°æ®é›†: {len(df)} æ ·æœ¬")

        # å¦‚æœè®¾ç½®äº†num_samplesï¼Œä¸”å°äºæ•°æ®é›†å¤§å°ï¼Œè¿›è¡Œé‡‡æ ·
        if args.num_samples and len(df) > args.num_samples * 2:
            print(f"âš ï¸  æ•°æ®é›†è¾ƒå¤§ï¼Œä¸ºåŠ å¿«é€Ÿåº¦éšæœºé‡‡æ · {args.num_samples * 2} æ ·æœ¬")
            import random
            random.seed(42)
            df = random.sample(df, args.num_samples * 2)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨user_dataé¿å…dataseté™åˆ¶ï¼‰
        train_loader, val_loader, test_loader, _ = get_train_val_loaders(
            dataset='user_data',
            dataset_array=df,
            target='target',
            n_train=None,
            n_val=None,
            n_test=None,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            batch_size=args.batch_size,
            atom_features='cgcnn',
            neighbor_strategy='k-nearest',
            line_graph=True,
            split_seed=42,
            workers=0,
            pin_memory=False,
            save_dataloader=False,
            filename='temp_attention_comparison',
            id_tag='jid',
            use_canonize=True,
            cutoff=8.0,
            max_neighbors=12,
            output_dir=args.save_dir
        )

        print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

    except Exception as e:
        print(f"âŒ  åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿:")
        print(f"  1. æ•°æ®é›†è·¯å¾„æ­£ç¡®: {args.root_dir}")
        print(f"  2. æ•°æ®é›†ç±»å‹æ­£ç¡®: {args.dataset}")
        print(f"  3. å±æ€§åç§°æ­£ç¡®: {args.property}")
        raise

    # æå–æ³¨æ„åŠ›æƒé‡
    results = comparator.extract_attention_weights(test_loader, args.num_samples)

    # å¯è§†åŒ–å¯¹æ¯”
    comparator.visualize_attention_comparison(results, args.save_dir)

    print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.save_dir}")


if __name__ == '__main__':
    main()
