#!/usr/bin/env python
"""
å¯è§†åŒ–ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡
å±•ç¤ºåŸå­-æ–‡æœ¬è¯ä¹‹é—´çš„å¯¹åº”å…³ç³»
"""

import os
import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from models.alignn import ALIGNN
from data import get_train_val_loaders

sns.set_style("white")
plt.rcParams['font.size'] = 9


def visualize_attention_heatmap(attn_weights, atoms, tokens, title, save_path,
                                cmap='YlOrRd', figsize=(12, 8)):
    """
    ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡çƒ­å›¾

    Args:
        attn_weights: [num_atoms, num_tokens] æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
        atoms: åŸå­åˆ—è¡¨
        tokens: æ–‡æœ¬tokenåˆ—è¡¨
        title: æ ‡é¢˜
        save_path: ä¿å­˜è·¯å¾„
    """
    fig, ax = plt.subplots(figsize=figsize)

    # ç»˜åˆ¶çƒ­å›¾
    im = ax.imshow(attn_weights, cmap=cmap, aspect='auto', vmin=0, vmax=1)

    # è®¾ç½®åæ ‡è½´
    ax.set_xticks(np.arange(len(tokens)))
    ax.set_yticks(np.arange(len(atoms)))
    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=10)
    ax.set_yticklabels(atoms, fontsize=10)

    ax.set_xlabel('Text Tokens', fontsize=12, weight='bold')
    ax.set_ylabel('Atoms', fontsize=12, weight='bold')
    ax.set_title(title, fontsize=14, weight='bold', pad=15)

    # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆåªæ˜¾ç¤ºæƒé‡>0.05çš„ï¼‰
    for i in range(len(atoms)):
        for j in range(len(tokens)):
            if attn_weights[i, j] > 0.05:
                text_color = 'white' if attn_weights[i, j] > 0.5 else 'black'
                ax.text(j, i, f'{attn_weights[i, j]:.2f}',
                       ha="center", va="center", color=text_color,
                       fontsize=8, weight='bold')

    # æ·»åŠ colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Attention Weight', rotation=270, labelpad=20, fontsize=11)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"  âœ… ä¿å­˜: {save_path}")
    plt.close()


def visualize_bidirectional_attention(atom_to_text, text_to_atom, atoms, tokens,
                                     material_name, save_path):
    """
    å¹¶æ’æ˜¾ç¤ºåŒå‘æ³¨æ„åŠ›

    Args:
        atom_to_text: [num_atoms, num_tokens] åŸå­â†’æ–‡æœ¬æ³¨æ„åŠ›
        text_to_atom: [num_tokens, num_atoms] æ–‡æœ¬â†’åŸå­æ³¨æ„åŠ›
        atoms: åŸå­åˆ—è¡¨
        tokens: æ–‡æœ¬tokenåˆ—è¡¨
        material_name: ææ–™åç§°
        save_path: ä¿å­˜è·¯å¾„
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # å·¦: åŸå­â†’æ–‡æœ¬
    im1 = ax1.imshow(atom_to_text, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)
    ax1.set_xticks(np.arange(len(tokens)))
    ax1.set_yticks(np.arange(len(atoms)))
    ax1.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)
    ax1.set_yticklabels(atoms, fontsize=9)
    ax1.set_xlabel('Text Tokens', fontsize=11, weight='bold')
    ax1.set_ylabel('Atoms', fontsize=11, weight='bold')
    ax1.set_title('Atom â†’ Token Attention\n(Which words does each atom focus on?)',
                 fontsize=12, weight='bold', pad=10)

    # æ ‡æ³¨
    for i in range(len(atoms)):
        for j in range(len(tokens)):
            if atom_to_text[i, j] > 0.05:
                color = 'white' if atom_to_text[i, j] > 0.5 else 'black'
                ax1.text(j, i, f'{atom_to_text[i, j]:.2f}',
                        ha="center", va="center", color=color, fontsize=7)

    plt.colorbar(im1, ax=ax1, label='Attention Weight')

    # å³: æ–‡æœ¬â†’åŸå­
    im2 = ax2.imshow(text_to_atom, cmap='YlGnBu', aspect='auto', vmin=0, vmax=1)
    ax2.set_xticks(np.arange(len(atoms)))
    ax2.set_yticks(np.arange(len(tokens)))
    ax2.set_xticklabels(atoms, rotation=45, ha='right', fontsize=9)
    ax2.set_yticklabels(tokens, fontsize=9)
    ax2.set_xlabel('Atoms', fontsize=11, weight='bold')
    ax2.set_ylabel('Text Tokens', fontsize=11, weight='bold')
    ax2.set_title('Token â†’ Atom Attention\n(Which atoms does each word focus on?)',
                 fontsize=12, weight='bold', pad=10)

    # æ ‡æ³¨
    for i in range(len(tokens)):
        for j in range(len(atoms)):
            if text_to_atom[i, j] > 0.05:
                color = 'white' if text_to_atom[i, j] > 0.5 else 'black'
                ax2.text(j, i, f'{text_to_atom[i, j]:.2f}',
                        ha="center", va="center", color=color, fontsize=7)

    plt.colorbar(im2, ax=ax2, label='Attention Weight')

    plt.suptitle(f'Fine-grained Cross-modal Attention: {material_name}',
                fontsize=14, weight='bold', y=1.02)
    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"  âœ… ä¿å­˜åŒå‘æ³¨æ„åŠ›å›¾: {save_path}")
    plt.close()


def extract_attention_examples(model, data_loader, device='cpu', num_examples=5):
    """
    æå–è‹¥å¹²ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–

    Returns:
        examples: list of dict, each containing:
            - 'material_id': ææ–™ID
            - 'text': æ–‡æœ¬æè¿°
            - 'tokens': tokenåˆ—è¡¨
            - 'atoms': åŸå­ç¬¦å·åˆ—è¡¨
            - 'atom_to_text': [num_atoms, num_tokens]
            - 'text_to_atom': [num_tokens, num_atoms]
    """
    print(f"ğŸ”„ æå–{num_examples}ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡...")

    model = model.to(device)
    model.eval()

    examples = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(data_loader, desc="æå–æ³¨æ„åŠ›")):
            if len(examples) >= num_examples:
                break

            if len(batch) == 3:
                g, text, target = batch
                lg = None
            elif len(batch) == 4:
                g, lg, text, target = batch
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„batchæ ¼å¼")

            g = g.to(device)
            if lg is not None:
                lg = lg.to(device)

            batch_size = len(text) if isinstance(text, list) else text.size(0)

            # æ„å»ºæ¨¡å‹è¾“å…¥
            if lg is not None:
                model_input = (g, lg, text)
            else:
                model_input = (g, text)

            # Forward with attention
            output = model(model_input, return_attention=True)

            if 'fine_grained_attention_weights' not in output:
                print("âš ï¸  æ¨¡å‹æœªå¯ç”¨ç»†ç²’åº¦æ³¨æ„åŠ›ï¼Œæ— æ³•æå–æ³¨æ„åŠ›æƒé‡")
                return []

            # è·å–æ³¨æ„åŠ›æƒé‡
            attn_weights = output['fine_grained_attention_weights']
            atom_to_text_attn = attn_weights['atom_to_text']  # [batch, heads, atoms, tokens]
            text_to_atom_attn = attn_weights['text_to_atom']  # [batch, heads, tokens, atoms]

            # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå¤„ç†
            batch_num_nodes = g.batch_num_nodes()

            for i in range(min(batch_size, num_examples - len(examples))):
                # è·å–è¯¥æ ·æœ¬çš„æ–‡æœ¬
                text_str = text[i] if isinstance(text, list) else None

                if text_str is None:
                    continue

                # ä»tokenizerè·å–tokensï¼ˆç®€åŒ–ï¼šç›´æ¥åˆ†è¯ï¼‰
                tokens = text_str.split()[:20]  # é™åˆ¶tokenæ•°é‡

                # è·å–åŸå­ä¿¡æ¯
                num_atoms = batch_num_nodes[i].item()

                # è·å–åŸå­ç¬¦å·ï¼ˆç®€åŒ–ï¼šä½¿ç”¨ç´¢å¼•ï¼‰
                atoms = [f"Atom{j+1}" for j in range(min(num_atoms, 30))]  # é™åˆ¶åŸå­æ•°é‡

                # å¹³å‡å¤šå¤´æ³¨æ„åŠ›
                atom_to_text = atom_to_text_attn[i].mean(dim=0)  # [atoms, tokens]
                text_to_atom = text_to_atom_attn[i].mean(dim=0)  # [tokens, atoms]

                # æˆªå–å¯¹åº”å¤§å°
                atom_to_text = atom_to_text[:len(atoms), :len(tokens)].cpu().numpy()
                text_to_atom = text_to_atom[:len(tokens), :len(atoms)].cpu().numpy()

                examples.append({
                    'material_id': f'Sample_{batch_idx}_{i}',
                    'text': text_str,
                    'tokens': tokens,
                    'atoms': atoms,
                    'atom_to_text': atom_to_text,
                    'text_to_atom': text_to_atom
                })

    print(f"âœ… æå–äº†{len(examples)}ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡")
    return examples


def analyze_attention_patterns(examples, save_dir):
    """
    åˆ†ææ³¨æ„åŠ›æ¨¡å¼

    ç»Ÿè®¡:
    - å¹³å‡æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
    - é«˜æƒé‡çš„åŸå­-è¯å¯¹
    - æ³¨æ„åŠ›ç¨€ç–åº¦
    """
    print("\nğŸ“Š åˆ†ææ³¨æ„åŠ›æ¨¡å¼...")

    report_path = os.path.join(save_dir, 'attention_analysis.txt')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡åˆ†ææŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")

        for idx, example in enumerate(examples):
            f.write(f"\n## æ ·æœ¬ {idx+1}: {example['material_id']}\n")
            f.write(f"æ–‡æœ¬æè¿°: {example['text'][:100]}...\n\n")

            atom_to_text = example['atom_to_text']
            text_to_atom = example['text_to_atom']

            # ç»Ÿè®¡
            f.write(f"åŸå­æ•°: {len(example['atoms'])}\n")
            f.write(f"Tokenæ•°: {len(example['tokens'])}\n")
            f.write(f"å¹³å‡æ³¨æ„åŠ›æƒé‡: {atom_to_text.mean():.4f}\n")
            f.write(f"æœ€å¤§æ³¨æ„åŠ›æƒé‡: {atom_to_text.max():.4f}\n")
            f.write(f"ç¨€ç–åº¦ (æƒé‡<0.1çš„æ¯”ä¾‹): {(atom_to_text < 0.1).mean():.2%}\n\n")

            # æ‰¾å‡ºtop-5é«˜æƒé‡çš„åŸå­-è¯å¯¹
            f.write("Top-5 åŸå­-è¯æ³¨æ„åŠ›å¯¹:\n")
            flat_indices = np.argsort(atom_to_text.flatten())[::-1][:5]
            for rank, flat_idx in enumerate(flat_indices, 1):
                i, j = np.unravel_index(flat_idx, atom_to_text.shape)
                weight = atom_to_text[i, j]
                atom = example['atoms'][i]
                token = example['tokens'][j]
                f.write(f"  {rank}. {atom} â† {token}: {weight:.4f}\n")

            f.write("\n" + "-"*80 + "\n")

    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def visualize_attention_distribution(examples, save_dir):
    """
    å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
    """
    print("\nğŸ“Š å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ...")

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # æ”¶é›†æ‰€æœ‰æ³¨æ„åŠ›æƒé‡
    all_weights = []
    for example in examples:
        all_weights.extend(example['atom_to_text'].flatten())

    # 1. ç›´æ–¹å›¾
    ax = axes[0, 0]
    ax.hist(all_weights, bins=50, edgecolor='black', alpha=0.7, color='skyblue')
    ax.set_xlabel('Attention Weight', fontsize=11)
    ax.set_ylabel('Frequency', fontsize=11)
    ax.set_title('Distribution of Attention Weights', fontsize=12, weight='bold')
    ax.grid(axis='y', alpha=0.3)

    # 2. CDF
    ax = axes[0, 1]
    sorted_weights = np.sort(all_weights)
    cdf = np.arange(1, len(sorted_weights)+1) / len(sorted_weights)
    ax.plot(sorted_weights, cdf, linewidth=2, color='coral')
    ax.set_xlabel('Attention Weight', fontsize=11)
    ax.set_ylabel('Cumulative Probability', fontsize=11)
    ax.set_title('Cumulative Distribution Function', fontsize=12, weight='bold')
    ax.grid(alpha=0.3)

    # 3. Boxplotï¼ˆæ¯ä¸ªæ ·æœ¬ï¼‰
    ax = axes[1, 0]
    box_data = [example['atom_to_text'].flatten() for example in examples]
    bp = ax.boxplot(box_data, labels=[f"S{i+1}" for i in range(len(examples))],
                    patch_artist=True)
    for patch in bp['boxes']:
        patch.set_facecolor('lightgreen')
    ax.set_xlabel('Sample', fontsize=11)
    ax.set_ylabel('Attention Weight', fontsize=11)
    ax.set_title('Attention Weight Distribution per Sample', fontsize=12, weight='bold')
    ax.grid(axis='y', alpha=0.3)

    # 4. ç»Ÿè®¡è¡¨
    ax = axes[1, 1]
    ax.axis('off')

    stats_data = [
        ['Metric', 'Value'],
        ['Mean', f'{np.mean(all_weights):.4f}'],
        ['Median', f'{np.median(all_weights):.4f}'],
        ['Std Dev', f'{np.std(all_weights):.4f}'],
        ['Min', f'{np.min(all_weights):.4f}'],
        ['Max', f'{np.max(all_weights):.4f}'],
        ['Sparsity (<0.1)', f'{(np.array(all_weights) < 0.1).mean():.2%}'],
        ['Num Samples', f'{len(examples)}'],
    ]

    table = ax.table(cellText=stats_data, cellLoc='left', loc='center',
                    colWidths=[0.5, 0.5])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(2):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')

    # äº¤æ›¿è¡Œé¢œè‰²
    for i in range(1, len(stats_data)):
        for j in range(2):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#E8F5E9')

    plt.suptitle('Attention Weight Statistics', fontsize=14, weight='bold')
    plt.tight_layout()

    save_path = os.path.join(save_dir, 'attention_distribution.pdf')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜åˆ†å¸ƒå›¾: {save_path}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['jarvis', 'mp', 'class'],
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--property', type=str, required=True,
                       help='å±æ€§åç§°')
    parser.add_argument('--root_dir', type=str, default='./dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')

    # å¯é€‰å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./attention_visualization',
                       help='ä¿å­˜ç»“æœçš„ç›®å½•')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ (cpu æˆ– cuda)')
    parser.add_argument('--num_examples', type=int, default=5,
                       help='å¯è§†åŒ–çš„æ ·æœ¬æ•°')
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--split', type=str, default='test',
                       choices=['train', 'val', 'test'],
                       help='ä½¿ç”¨å“ªä¸ªæ•°æ®é›†split')

    args = parser.parse_args()

    os.makedirs(args.save_dir, exist_ok=True)

    print("="*80)
    print("ğŸ¨ ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–")
    print("="*80)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Dataset: {args.dataset}/{args.property}")
    print(f"Save dir: {args.save_dir}")
    print()

    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(args.checkpoint, map_location='cpu', weights_only=False)
    model_config = checkpoint.get('config', None)

    if model_config is None:
        print("âŒ Checkpointä¸­æœªæ‰¾åˆ°æ¨¡å‹é…ç½®")
        return

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç»†ç²’åº¦æ³¨æ„åŠ›
    if not getattr(model_config, 'use_fine_grained_attention', False):
        print("âŒ æ¨¡å‹æœªå¯ç”¨ç»†ç²’åº¦æ³¨æ„åŠ›ï¼ˆuse_fine_grained_attention=Falseï¼‰")
        print("   è¯·ä½¿ç”¨å¯ç”¨äº†ç»†ç²’åº¦æ³¨æ„åŠ›çš„æ¨¡å‹")
        return

    model = ALIGNN(model_config)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"âœ… ç»†ç²’åº¦æ³¨æ„åŠ›å·²å¯ç”¨")

    # 2. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    from train_with_cross_modal_attention import load_dataset, get_dataset_paths

    dataset_mapping = {'jarvis': 'jarvis', 'mp': 'mp', 'class': 'class'}
    actual_dataset = dataset_mapping.get(args.dataset.lower(), args.dataset.lower())

    cif_dir, id_prop_file = get_dataset_paths(args.root_dir, actual_dataset, args.property)
    df = load_dataset(cif_dir, id_prop_file, actual_dataset, args.property)
    print(f"âœ… æ•°æ®é›†å¤§å°: {len(df)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    (train_loader, val_loader, test_loader, _) = get_train_val_loaders(
        dataset='user_data',
        dataset_array=df,
        target='target',
        batch_size=args.batch_size,
        atom_features=model_config.atom_features if hasattr(model_config, 'atom_features') else 'cgcnn',
        neighbor_strategy='k-nearest',
        line_graph=model_config.line_graph if hasattr(model_config, 'line_graph') else True,
        split_seed=42,
        workers=4,
        pin_memory=False,
        save_dataloader=False,
        filename='temp',
        id_tag='jid',
        use_canonize=True,
        cutoff=8.0,
        max_neighbors=12,
        output_dir=args.save_dir
    )

    # é€‰æ‹©æ•°æ®é›†
    if args.split == 'train':
        data_loader = train_loader
    elif args.split == 'val':
        data_loader = val_loader
    else:
        data_loader = test_loader

    print(f"âœ… ä½¿ç”¨{args.split}é›†")

    # 3. æå–æ³¨æ„åŠ›æƒé‡
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = 'cpu'

    examples = extract_attention_examples(model, data_loader, device=device,
                                         num_examples=args.num_examples)

    if not examples:
        print("âŒ æœªèƒ½æå–æ³¨æ„åŠ›æƒé‡")
        return

    # 4. å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬
    print(f"\nğŸ¨ å¯è§†åŒ–{len(examples)}ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›...")
    for idx, example in enumerate(examples):
        print(f"\n  å¤„ç†æ ·æœ¬ {idx+1}/{len(examples)}: {example['material_id']}")

        # åŒå‘æ³¨æ„åŠ›
        save_path = os.path.join(args.save_dir,
                                f"attention_sample_{idx+1}_bidirectional.pdf")
        visualize_bidirectional_attention(
            example['atom_to_text'],
            example['text_to_atom'],
            example['atoms'],
            example['tokens'],
            example['material_id'],
            save_path
        )

    # 5. åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    analyze_attention_patterns(examples, args.save_dir)

    # 6. å¯è§†åŒ–æ³¨æ„åŠ›åˆ†å¸ƒ
    visualize_attention_distribution(examples, args.save_dir)

    print("\n" + "="*80)
    print("âœ… æ³¨æ„åŠ›å¯è§†åŒ–å®Œæˆï¼")
    print("="*80)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶åœ¨: {args.save_dir}")
    print(f"  - attention_sample_*_bidirectional.pdf/png: åŒå‘æ³¨æ„åŠ›çƒ­å›¾")
    print(f"  - attention_distribution.pdf/png: æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ")
    print(f"  - attention_analysis.txt: æ³¨æ„åŠ›æ¨¡å¼åˆ†ææŠ¥å‘Š")


if __name__ == '__main__':
    main()
