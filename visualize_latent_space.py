#!/usr/bin/env python
"""
æ½œåœ¨ç©ºé—´å¯è§†åŒ– (Latent Space Visualization)
ä½¿ç”¨ t-SNE å’Œ UMAP å°†æ¨¡å‹å­¦ä¹ åˆ°çš„ç‰¹å¾åµŒå…¥é™ç»´åˆ° 2D/3D ç©ºé—´
å±•ç¤ºèåˆè¡¨ç¤ºçš„è´¨é‡å’Œä¸åŒææ–™çš„åŒºåˆ†èƒ½åŠ›
"""

import os
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# é™ç»´ç®—æ³•
from sklearn.manifold import TSNE
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("âš ï¸  UMAPæœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨t-SNEã€‚å®‰è£…æ–¹æ³•: pip install umap-learn")

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("white")
plt.rcParams['font.size'] = 11
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['figure.titlesize'] = 16


def load_model_and_data(checkpoint_path, config_file=None):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    # ä»checkpointä¸­æ¢å¤æ¨¡å‹é…ç½®
    model_config = checkpoint.get('config', None)
    if model_config is None:
        raise ValueError("Checkpointä¸­æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ä¿¡æ¯")

    # é‡æ–°æ„å»ºæ¨¡å‹
    from models.alignn import ALIGNN
    model = ALIGNN(model_config)
    model.load_state_dict(checkpoint['model'])
    model.eval()

    return model, model_config


def extract_features(model, data_loader, device='cpu', feature_types=['fused']):
    """
    ä»æ¨¡å‹ä¸­æå–ä¸­é—´ç‰¹å¾

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        feature_types: è¦æå–çš„ç‰¹å¾ç±»å‹åˆ—è¡¨
                      ['graph', 'text', 'fused'] æˆ–å…¶å­é›†

    Returns:
        features_dict: {feature_type: features_array}
        targets: ç›®æ ‡å€¼æ•°ç»„
    """
    model = model.to(device)
    model.eval()

    features_dict = {ft: [] for ft in feature_types}
    targets = []

    print(f"ğŸ”„ æ­£åœ¨æå–ç‰¹å¾: {feature_types}")

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="æå–ç‰¹å¾"):
            # è§£åŒ…batch
            if len(batch) == 3:
                g, text, target = batch
            elif len(batch) == 4:
                g, lg, text, target = batch
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„batchæ ¼å¼: {len(batch)}ä¸ªå…ƒç´ ")

            g = g.to(device)

            # å¤„ç†textè¾“å…¥ï¼ˆå¯èƒ½æ˜¯dict, tensor, æˆ–listï¼‰
            if isinstance(text, dict):
                text = {k: v.to(device) for k, v in text.items()}
            elif isinstance(text, (list, tuple)):
                # å¦‚æœæ˜¯list/tupleï¼Œæ¯ä¸ªå…ƒç´ å¯èƒ½æ˜¯tensor
                text_processed = []
                for item in text:
                    if isinstance(item, dict):
                        text_processed.append({k: v.to(device) for k, v in item.items()})
                    elif torch.is_tensor(item):
                        text_processed.append(item.to(device))
                    else:
                        text_processed.append(item)
                text = text_processed
            elif torch.is_tensor(text):
                text = text.to(device)

            # Pack inputs for ALIGNN model
            # ALIGNN expects: (g, lg, text) or just g depending on batch format
            if len(batch) == 4:
                # Batch format: (g, lg, text, target)
                # Model expects: forward((g, lg, text), return_features=True)
                model_input = (g, batch[1].to(device), text)
            else:
                # Batch format: (g, text, target)
                # Model expects: forward((g, text), return_features=True)
                model_input = (g, text)

            # Forward pass with return_features=True
            output = model(model_input, return_features=True)

            if isinstance(output, dict):
                # æå–ä¸åŒç±»å‹çš„ç‰¹å¾
                if 'graph' in feature_types and 'graph_features' in output:
                    features_dict['graph'].append(output['graph_features'].cpu().numpy())

                if 'text' in feature_types and 'text_features' in output:
                    features_dict['text'].append(output['text_features'].cpu().numpy())

                if 'fused' in feature_types:
                    # èåˆç‰¹å¾æ˜¯graphå’Œtextç‰¹å¾çš„ç»„åˆ
                    if 'graph_features' in output and 'text_features' in output:
                        graph_feat = output['graph_features'].cpu().numpy()
                        text_feat = output['text_features'].cpu().numpy()
                        fused_feat = np.concatenate([graph_feat, text_feat], axis=1)
                        features_dict['fused'].append(fused_feat)

            targets.append(target.cpu().numpy())

    # åˆå¹¶æ‰€æœ‰batch
    for ft in feature_types:
        if features_dict[ft]:
            features_dict[ft] = np.vstack(features_dict[ft])
        else:
            print(f"âš ï¸  æœªèƒ½æå– {ft} ç‰¹å¾")
            features_dict[ft] = None

    targets = np.concatenate(targets)

    print(f"âœ… ç‰¹å¾æå–å®Œæˆ")
    for ft, feat in features_dict.items():
        if feat is not None:
            print(f"   {ft}: {feat.shape}")
    print(f"   targets: {targets.shape}")

    return features_dict, targets


def apply_dimensionality_reduction(features, method='tsne', n_components=2, **kwargs):
    """
    åº”ç”¨é™ç»´ç®—æ³•

    Args:
        features: ç‰¹å¾çŸ©é˜µ [n_samples, n_features]
        method: 'tsne' æˆ– 'umap'
        n_components: é™ç»´åˆ°çš„ç»´åº¦ (2 æˆ– 3)
        **kwargs: ä¼ é€’ç»™é™ç»´ç®—æ³•çš„å…¶ä»–å‚æ•°

    Returns:
        embedded: é™ç»´åçš„ç‰¹å¾ [n_samples, n_components]
    """
    print(f"ğŸ”„ åº”ç”¨{method.upper()}é™ç»´åˆ°{n_components}D...")

    if method.lower() == 'tsne':
        default_params = {
            'n_components': n_components,
            'perplexity': min(30, len(features) - 1),
            'max_iter': 1000,  # æ–°ç‰ˆsklearnä½¿ç”¨max_iterè€Œä¸æ˜¯n_iter
            'random_state': 42,
            'verbose': 0
        }
        default_params.update(kwargs)
        reducer = TSNE(**default_params)

    elif method.lower() == 'umap':
        if not UMAP_AVAILABLE:
            raise ImportError("UMAPæœªå®‰è£…ï¼Œè¯·ä½¿ç”¨: pip install umap-learn")

        default_params = {
            'n_components': n_components,
            'n_neighbors': min(15, len(features) - 1),
            'min_dist': 0.1,
            'random_state': 42,
            'verbose': False
        }
        default_params.update(kwargs)
        reducer = umap.UMAP(**default_params)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")

    embedded = reducer.fit_transform(features)
    print(f"âœ… é™ç»´å®Œæˆ: {embedded.shape}")

    return embedded


def plot_2d_embedding(embedded, targets, title, save_path, is_classification=False):
    """ç»˜åˆ¶2DåµŒå…¥ç©ºé—´"""
    fig, ax = plt.subplots(figsize=(10, 8))

    if is_classification:
        # åˆ†ç±»ä»»åŠ¡ï¼šæŒ‰ç±»åˆ«ç€è‰²
        unique_classes = np.unique(targets)
        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_classes)))

        for idx, cls in enumerate(unique_classes):
            mask = targets == cls
            ax.scatter(embedded[mask, 0], embedded[mask, 1],
                      c=[colors[idx]], label=f'Class {int(cls)}',
                      alpha=0.7, s=50, edgecolors='k', linewidth=0.5)
        ax.legend(loc='best')
    else:
        # å›å½’ä»»åŠ¡ï¼šæŒ‰å€¼ç€è‰²
        scatter = ax.scatter(embedded[:, 0], embedded[:, 1],
                           c=targets, cmap='viridis', alpha=0.7,
                           s=50, edgecolors='k', linewidth=0.5)
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Target Value', rotation=270, labelpad=20)

    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_title(title)
   # ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜2Då›¾: {save_path}")
    plt.close()


def plot_3d_embedding(embedded, targets, title, save_path, is_classification=False):
    """ç»˜åˆ¶3DåµŒå…¥ç©ºé—´"""
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')

    if is_classification:
        # åˆ†ç±»ä»»åŠ¡ï¼šæŒ‰ç±»åˆ«ç€è‰²
        unique_classes = np.unique(targets)
        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_classes)))

        for idx, cls in enumerate(unique_classes):
            mask = targets == cls
            ax.scatter(embedded[mask, 0], embedded[mask, 1], embedded[mask, 2],
                      c=[colors[idx]], label=f'Class {int(cls)}',
                      alpha=0.7, s=50, edgecolors='k', linewidth=0.5)
        ax.legend(loc='best')
    else:
        # å›å½’ä»»åŠ¡ï¼šæŒ‰å€¼ç€è‰²
        scatter = ax.scatter(embedded[:, 0], embedded[:, 1], embedded[:, 2],
                           c=targets, cmap='viridis', alpha=0.7,
                           s=50, edgecolors='k', linewidth=0.5)
        cbar = plt.colorbar(scatter, ax=ax, pad=0.1)
        cbar.set_label('Target Value', rotation=270, labelpad=20)

    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_zlabel('Dimension 3')
    ax.set_title(title)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜3Då›¾: {save_path}")
    plt.close()


def plot_comparison(embeddings_dict, targets, method, save_path, is_classification=False):
    """
    å¹¶æ’å¯¹æ¯”ä¸åŒç‰¹å¾ç±»å‹çš„åµŒå…¥ç©ºé—´

    Args:
        embeddings_dict: {feature_type: embedded_2d}
        targets: ç›®æ ‡å€¼
        method: é™ç»´æ–¹æ³•åç§°
        save_path: ä¿å­˜è·¯å¾„
        is_classification: æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
    """
    n_plots = len(embeddings_dict)
    fig, axes = plt.subplots(1, n_plots, figsize=(7*n_plots, 6))

    if n_plots == 1:
        axes = [axes]

    titles = {
        'graph': 'Graph Features Only',
        'text': 'Text Features Only',
        'fused': 'Fused Features (Graph + Text)'
    }

    # è®¡ç®—æ‰€æœ‰åµŒå…¥çš„å…¨å±€åæ ‡èŒƒå›´ï¼Œç”¨äºç»Ÿä¸€æ¨ªçºµåæ ‡
    all_embeddings = np.vstack(list(embeddings_dict.values()))
    x_min, x_max = all_embeddings[:, 0].min(), all_embeddings[:, 0].max()
    y_min, y_max = all_embeddings[:, 1].min(), all_embeddings[:, 1].max()

    # æ·»åŠ 10%çš„padding
    x_margin = (x_max - x_min) * 0.1
    y_margin = (y_max - y_min) * 0.1
    x_lim = [x_min - x_margin, x_max + x_margin]
    y_lim = [y_min - y_margin, y_max + y_margin]

    for idx, (feat_type, embedded) in enumerate(embeddings_dict.items()):
        ax = axes[idx]

        if is_classification:
            # åˆ†ç±»ä»»åŠ¡
            unique_classes = np.unique(targets)
            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_classes)))

            for cls_idx, cls in enumerate(unique_classes):
                mask = targets == cls
                ax.scatter(embedded[mask, 0], embedded[mask, 1],
                          c=[colors[cls_idx]], label=f'Class {int(cls)}',
                          alpha=0.7, s=40, edgecolors='k', linewidth=0.5)
            ax.legend(loc='best', fontsize=9)
        else:
            # å›å½’ä»»åŠ¡
            scatter = ax.scatter(embedded[:, 0], embedded[:, 1],
                               c=targets, cmap='viridis', alpha=0.7,
                               s=40, edgecolors='k', linewidth=0.5)
            if idx == n_plots - 1:  # åªåœ¨æœ€åä¸€ä¸ªå­å›¾æ·»åŠ colorbar
                cbar = plt.colorbar(scatter, ax=ax)
                cbar.set_label('Target Value', rotation=270, labelpad=15)

        # ç»Ÿä¸€è®¾ç½®æ¨ªçºµåæ ‡èŒƒå›´
        ax.set_xlim(x_lim)
        ax.set_ylim(y_lim)

        ax.set_xlabel('Dimension 1')
        ax.set_ylabel('Dimension 2')
        ax.set_title(titles.get(feat_type, feat_type))
       # ax.grid(True, alpha=0.3)

    plt.suptitle(f'Latent Space Comparison ({method.upper()})', fontsize=16, y=1.02)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜å¯¹æ¯”å›¾: {save_path}")
    plt.close()


def visualize_latent_space(checkpoint_path, data_loader, save_dir, device='cpu',
                           feature_types=['graph', 'text', 'fused'],
                           methods=['tsne', 'umap'], dimensions=[2, 3],
                           is_classification=False):
    """
    å®Œæ•´çš„æ½œåœ¨ç©ºé—´å¯è§†åŒ–æµç¨‹

    Args:
        checkpoint_path: æ¨¡å‹checkpointè·¯å¾„
        data_loader: æ•°æ®åŠ è½½å™¨
        save_dir: ä¿å­˜ç›®å½•
        device: è®¾å¤‡
        feature_types: è¦å¯è§†åŒ–çš„ç‰¹å¾ç±»å‹
        methods: é™ç»´æ–¹æ³•
        dimensions: é™ç»´ç»´åº¦
        is_classification: æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
    """
    os.makedirs(save_dir, exist_ok=True)

    print("="*70)
    print("ğŸ¨ æ½œåœ¨ç©ºé—´å¯è§†åŒ–")
    print("="*70)

    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    model, model_config = load_model_and_data(checkpoint_path)
    model = model.to(device)

    # 2. æå–ç‰¹å¾
    features_dict, targets = extract_features(model, data_loader, device, feature_types)

    # è¿‡æ»¤æ‰Noneçš„ç‰¹å¾
    features_dict = {k: v for k, v in features_dict.items() if v is not None}

    if not features_dict:
        print("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾ï¼Œé€€å‡º")
        return

    # 3. å¯¹æ¯ç§ç‰¹å¾ç±»å‹å’Œé™ç»´æ–¹æ³•è¿›è¡Œå¯è§†åŒ–
    embeddings_2d = {}

    for method in methods:
        if method == 'umap' and not UMAP_AVAILABLE:
            print(f"âš ï¸  è·³è¿‡{method.upper()}ï¼ˆæœªå®‰è£…ï¼‰")
            continue

        print(f"\n{'='*70}")
        print(f"ğŸ“Š ä½¿ç”¨ {method.upper()} è¿›è¡Œé™ç»´")
        print(f"{'='*70}")

        embeddings_2d_method = {}

        for feat_type, features in features_dict.items():
            print(f"\n--- å¤„ç† {feat_type} ç‰¹å¾ ---")

            # 2Dé™ç»´
            if 2 in dimensions:
                embedded_2d = apply_dimensionality_reduction(
                    features, method=method, n_components=2
                )
                embeddings_2d_method[feat_type] = embedded_2d

                # å•ç‹¬ç»˜åˆ¶
                save_path = os.path.join(save_dir,
                    f'latent_space_{feat_type}_{method}_2d.pdf')
                title = f'{feat_type.capitalize()} Features - {method.upper()} 2D'
                plot_2d_embedding(embedded_2d, targets, title, save_path, is_classification)

            # 3Dé™ç»´
            if 3 in dimensions:
                embedded_3d = apply_dimensionality_reduction(
                    features, method=method, n_components=3
                )

                save_path = os.path.join(save_dir,
                    f'latent_space_{feat_type}_{method}_3d.pdf')
                title = f'{feat_type.capitalize()} Features - {method.upper()} 3D'
                plot_3d_embedding(embedded_3d, targets, title, save_path, is_classification)

        # ç»˜åˆ¶å¯¹æ¯”å›¾ï¼ˆä»…2Dï¼‰
        if embeddings_2d_method and 2 in dimensions:
            comparison_path = os.path.join(save_dir,
                f'latent_space_comparison_{method}_2d.pdf')
            plot_comparison(embeddings_2d_method, targets, method,
                          comparison_path, is_classification)

    print("\n" + "="*70)
    print("âœ… æ½œåœ¨ç©ºé—´å¯è§†åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"æ‰€æœ‰å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_dir}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ½œåœ¨ç©ºé—´å¯è§†åŒ–')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='æ¨¡å‹checkpointè·¯å¾„ï¼ˆå¦‚ best_val_model.ptï¼‰')
    parser.add_argument('--data_loader_path', type=str, required=True,
                        help='ä¿å­˜çš„data loaderè·¯å¾„ï¼ˆå¦‚ data_loader_test.ptï¼‰')

    # å¯é€‰å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./latent_space_vis',
                        help='ä¿å­˜å›¾ç‰‡çš„ç›®å½•')
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¾å¤‡ (cpu æˆ– cuda)')
    parser.add_argument('--feature_types', nargs='+',
                        default=['graph', 'text', 'fused'],
                        choices=['graph', 'text', 'fused'],
                        help='è¦å¯è§†åŒ–çš„ç‰¹å¾ç±»å‹')
    parser.add_argument('--methods', nargs='+', default=['tsne', 'umap'],
                        choices=['tsne', 'umap'],
                        help='é™ç»´æ–¹æ³•')
    parser.add_argument('--dimensions', nargs='+', type=int, default=[2, 3],
                        choices=[2, 3],
                        help='é™ç»´ç»´åº¦')
    parser.add_argument('--classification', action='store_true',
                        help='æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœéœ€è¦é‡æ–°åˆ›å»ºdata loaderï¼‰')

    args = parser.parse_args()

    # åŠ è½½data loader
    print(f"ğŸ“¥ åŠ è½½æ•°æ®...")
    if os.path.exists(args.data_loader_path):
        data_loader = torch.load(args.data_loader_path, weights_only=False)
        print(f"âœ… ä»æ–‡ä»¶åŠ è½½data loader: {args.data_loader_path}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°data loaderæ–‡ä»¶: {args.data_loader_path}")
        print("æç¤º: è®­ç»ƒæ—¶è®¾ç½® --save_dataloader å‚æ•°æ¥ä¿å­˜data loader")
        exit(1)

    # æ£€æŸ¥è®¾å¤‡
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = 'cpu'

    # è¿è¡Œå¯è§†åŒ–
    visualize_latent_space(
        checkpoint_path=args.checkpoint,
        data_loader=data_loader,
        save_dir=args.save_dir,
        device=device,
        feature_types=args.feature_types,
        methods=args.methods,
        dimensions=args.dimensions,
        is_classification=args.classification
    )
