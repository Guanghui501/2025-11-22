#!/usr/bin/env python
"""
å®Œæ•´å¯è§£é‡Šæ€§åˆ†ææ¼”ç¤º - è¶…è¶Šå•åŸå­åˆ†æ

å±•ç¤ºå¦‚ä½•ç»¼åˆåˆ†æï¼š
1. åŸå­é‡è¦æ€§
2. è¾¹ï¼ˆåŒ–å­¦é”®ï¼‰é‡è¦æ€§
3. è§’åº¦/ä¸‰å…ƒç»„é‡è¦æ€§
4. é…ä½ç¯å¢ƒ
5. å­ç»“æ„/åŸºåº
6. è·¨æ¨¡æ€æ³¨æ„åŠ›

æä¾›ææ–™å±æ€§é¢„æµ‹çš„å…¨æ–¹ä½è§£é‡Š
"""

import torch
import numpy as np
from pathlib import Path

from interpretability_enhanced import EnhancedInterpretabilityAnalyzer
from interpretability_graph_structure import GraphStructureAnalyzer


def complete_interpretability_analysis(
    model,
    g, lg, text,
    atoms_object,
    true_value=None,
    save_dir='./complete_analysis',
    sample_id='sample'
):
    """
    å®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†ææµç¨‹

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        g, lg, text: æ¨¡å‹è¾“å…¥
        atoms_object: Atomså¯¹è±¡
        true_value: çœŸå®å€¼
        save_dir: ä¿å­˜ç›®å½•
        sample_id: æ ·æœ¬ID

    Returns:
        complete_report: å®Œæ•´åˆ†ææŠ¥å‘Š
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    device = next(model.parameters()).device

    print("\n" + "="*80)
    print("ğŸ”¬ å®Œæ•´å¯è§£é‡Šæ€§åˆ†æ")
    print("="*80)
    print(f"æ ·æœ¬ID: {sample_id}")
    print(f"åŸå­æ•°: {atoms_object.num_atoms}")
    print(f"åŒ–å­¦å¼: {atoms_object.composition.reduced_formula}")
    print("="*80 + "\n")

    # ========== ç¬¬1éƒ¨åˆ†: é¢„æµ‹å’Œè·¨æ¨¡æ€æ³¨æ„åŠ› ==========
    print("ã€ç¬¬1éƒ¨åˆ†ã€‘é¢„æµ‹å’Œè·¨æ¨¡æ€æ³¨æ„åŠ›åˆ†æ")
    print("-" * 80)

    analyzer = EnhancedInterpretabilityAnalyzer(model, device=device)

    # æå–æ³¨æ„åŠ›å’Œé¢„æµ‹
    result = analyzer.extract_attention_weights(g, lg, text, return_prediction=True)

    prediction = result['prediction'][0] if len(result['prediction'].shape) > 0 else result['prediction']

    print(f"\nğŸ“Š é¢„æµ‹ç»“æœ:")
    print(f"   é¢„æµ‹å€¼: {prediction:.4f}")
    if true_value is not None:
        error = abs(prediction - true_value)
        print(f"   çœŸå®å€¼: {true_value:.4f}")
        print(f"   è¯¯å·®: {error:.4f}")
        print(f"   ç›¸å¯¹è¯¯å·®: {100*error/abs(true_value):.2f}%")

    # å¯è§†åŒ–æ³¨æ„åŠ›
    if result['attention_weights'] is not None:
        print(f"\nğŸ” è·¨æ¨¡æ€æ³¨æ„åŠ›:")
        attn = result['attention_weights']

        if 'graph_to_text' in attn and attn['graph_to_text'] is not None:
            g2t_mean = attn['graph_to_text'].mean().item()
            print(f"   Graphâ†’Text: {g2t_mean:.4f} (å›¾å¯¹æ–‡æœ¬çš„ä¾èµ–)")

        if 'text_to_graph' in attn and attn['text_to_graph'] is not None:
            t2g_mean = attn['text_to_graph'].mean().item()
            print(f"   Textâ†’Graph: {t2g_mean:.4f} (æ–‡æœ¬å¯¹å›¾çš„ä¾èµ–)")

        analyzer.visualize_cross_modal_attention(
            result['attention_weights'],
            save_path=save_dir / f'{sample_id}_attention.png'
        )

        analyzer.visualize_attention_by_heads(
            result['attention_weights'],
            save_path=save_dir / f'{sample_id}_attention_heads.png'
        )

    # ========== ç¬¬2éƒ¨åˆ†: åŸå­é‡è¦æ€§ ==========
    print("\n" + "="*80)
    print("ã€ç¬¬2éƒ¨åˆ†ã€‘åŸå­é‡è¦æ€§åˆ†æ")
    print("-" * 80)

    # æ¢¯åº¦æ³•
    atom_importance = analyzer.compute_atom_importance(g, lg, text, method='gradient')

    print(f"\nğŸ“Š åŸå­é‡è¦æ€§ç»Ÿè®¡:")
    print(f"   å¹³å‡å€¼: {atom_importance.mean():.4f}")
    print(f"   æ ‡å‡†å·®: {atom_importance.std():.4f}")
    print(f"   èŒƒå›´: [{atom_importance.min():.4f}, {atom_importance.max():.4f}]")

    # å¯è§†åŒ–
    atom_df = analyzer.visualize_atom_importance(
        atoms_object,
        atom_importance,
        save_path=save_dir / f'{sample_id}_atom_importance.png',
        top_k=10
    )

    # ========== ç¬¬3éƒ¨åˆ†: è¾¹ï¼ˆåŒ–å­¦é”®ï¼‰é‡è¦æ€§ ==========
    print("\n" + "="*80)
    print("ã€ç¬¬3éƒ¨åˆ†ã€‘è¾¹ï¼ˆåŒ–å­¦é”®ï¼‰é‡è¦æ€§åˆ†æ")
    print("-" * 80)

    struct_analyzer = GraphStructureAnalyzer(model, device=device)

    edge_importance, edge_info = struct_analyzer.compute_edge_importance(g, lg, text)

    print(f"\nğŸ“Š è¾¹é‡è¦æ€§ç»Ÿè®¡:")
    print(f"   è¾¹æ•°é‡: {len(edge_importance)}")
    print(f"   å¹³å‡å€¼: {edge_importance.mean():.4f}")
    print(f"   æ ‡å‡†å·®: {edge_importance.std():.4f}")

    edge_df = struct_analyzer.visualize_edge_importance(
        g, atoms_object, edge_importance, edge_info,
        save_path=save_dir / f'{sample_id}_edge_importance.png',
        top_k=20
    )

    # ========== ç¬¬4éƒ¨åˆ†: è§’åº¦/ä¸‰å…ƒç»„é‡è¦æ€§ ==========
    print("\n" + "="*80)
    print("ã€ç¬¬4éƒ¨åˆ†ã€‘è§’åº¦/ä¸‰å…ƒç»„é‡è¦æ€§åˆ†æï¼ˆALIGNNç‰¹æœ‰ï¼‰")
    print("-" * 80)

    angle_importance, angle_info = struct_analyzer.compute_angle_importance(g, lg, text)

    if angle_importance is not None:
        print(f"\nğŸ“Š è§’åº¦é‡è¦æ€§ç»Ÿè®¡:")
        print(f"   è§’åº¦æ•°é‡: {len(angle_importance)}")
        print(f"   å¹³å‡å€¼: {angle_importance.mean():.4f}")
        print(f"   æ ‡å‡†å·®: {angle_importance.std():.4f}")
    else:
        print("\nâš ï¸  è§’åº¦ä¿¡æ¯ä¸å¯ç”¨")

    # ========== ç¬¬5éƒ¨åˆ†: é…ä½ç¯å¢ƒåˆ†æ ==========
    print("\n" + "="*80)
    print("ã€ç¬¬5éƒ¨åˆ†ã€‘é…ä½ç¯å¢ƒåˆ†æ")
    print("-" * 80)

    coord_analysis = struct_analyzer.analyze_coordination_environment(
        g, atoms_object, atom_importance
    )

    coord_df = struct_analyzer.visualize_coordination_environment(
        coord_analysis,
        save_path=save_dir / f'{sample_id}_coordination.png',
        top_k=15
    )

    # é…ä½ç¯å¢ƒç»Ÿè®¡
    coord_nums = [c['coordination_number'] for c in coord_analysis]
    print(f"\nğŸ“Š é…ä½ç¯å¢ƒç»Ÿè®¡:")
    print(f"   å¹³å‡é…ä½æ•°: {np.mean(coord_nums):.2f}")
    print(f"   é…ä½æ•°èŒƒå›´: [{min(coord_nums)}, {max(coord_nums)}]")

    # ========== ç¬¬6éƒ¨åˆ†: é‡è¦å­ç»“æ„è¯†åˆ« ==========
    print("\n" + "="*80)
    print("ã€ç¬¬6éƒ¨åˆ†ã€‘é‡è¦å­ç»“æ„/åŸºåºè¯†åˆ«")
    print("-" * 80)

    substructures = struct_analyzer.identify_important_substructures(
        g, atoms_object, atom_importance, edge_importance,
        subgraph_size=3, top_k=20
    )

    struct_analyzer.visualize_substructures(
        substructures,
        save_path=save_dir / f'{sample_id}_substructures.png'
    )

    # ========== ç¬¬7éƒ¨åˆ†: ç‰¹å¾ç©ºé—´ï¼ˆå¦‚æœæœ‰å¤šæ ·æœ¬ï¼‰==========
    if result['graph_features'] is not None and result['text_features'] is not None:
        print("\n" + "="*80)
        print("ã€ç¬¬7éƒ¨åˆ†ã€‘ç‰¹å¾ç©ºé—´åˆ†æ")
        print("-" * 80)
        print("   ï¼ˆéœ€è¦å¤šä¸ªæ ·æœ¬æ‰èƒ½æœ‰æ•ˆå¯è§†åŒ–ï¼‰")

    # ========== ç”Ÿæˆç»¼åˆæŠ¥å‘Š ==========
    print("\n" + "="*80)
    print("ğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
    print("="*80)

    # ç»¼åˆæ´å¯Ÿ
    insights = generate_insights(
        prediction, true_value,
        atom_importance, atom_df,
        edge_importance, edge_df,
        coord_analysis,
        substructures,
        result['attention_weights']
    )

    # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
    report_path = save_dir / f'{sample_id}_analysis_report.txt'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write(f"å®Œæ•´å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š - {sample_id}\n")
        f.write("="*80 + "\n\n")

        f.write(f"æ ·æœ¬ä¿¡æ¯:\n")
        f.write(f"  åŒ–å­¦å¼: {atoms_object.composition.reduced_formula}\n")
        f.write(f"  åŸå­æ•°: {atoms_object.num_atoms}\n")
        f.write(f"  è¾¹æ•°: {g.num_edges()}\n\n")

        f.write(f"é¢„æµ‹ç»“æœ:\n")
        f.write(f"  é¢„æµ‹å€¼: {prediction:.4f}\n")
        if true_value is not None:
            f.write(f"  çœŸå®å€¼: {true_value:.4f}\n")
            f.write(f"  è¯¯å·®: {abs(prediction - true_value):.4f}\n\n")

        f.write("\n" + "="*80 + "\n")
        f.write("å…³é”®æ´å¯Ÿ\n")
        f.write("="*80 + "\n\n")

        for i, insight in enumerate(insights, 1):
            f.write(f"{i}. {insight}\n\n")

    print(f"\nâœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # åˆ›å»ºæ€»ç»“
    complete_report = {
        'sample_id': sample_id,
        'prediction': float(prediction),
        'true_value': float(true_value) if true_value is not None else None,
        'num_atoms': atoms_object.num_atoms,
        'num_edges': g.num_edges(),
        'insights': insights,
        'files_generated': [
            f'{sample_id}_attention.png',
            f'{sample_id}_attention_heads.png',
            f'{sample_id}_atom_importance.png',
            f'{sample_id}_edge_importance.png',
            f'{sample_id}_coordination.png',
            f'{sample_id}_substructures.png',
            f'{sample_id}_analysis_report.txt'
        ]
    }

    print(f"\n{'='*80}")
    print(f"âœ… å®Œæ•´åˆ†æå®Œæˆï¼")
    print(f"   ç»“æœä¿å­˜åœ¨: {save_dir}")
    print(f"   ç”Ÿæˆäº† {len(complete_report['files_generated'])} ä¸ªæ–‡ä»¶")
    print(f"{'='*80}\n")

    return complete_report


def generate_insights(
    prediction, true_value,
    atom_importance, atom_df,
    edge_importance, edge_df,
    coord_analysis,
    substructures,
    attention_weights
):
    """
    ä»åˆ†æç»“æœä¸­ç”Ÿæˆå…³é”®æ´å¯Ÿ

    Returns:
        insights: æ´å¯Ÿåˆ—è¡¨
    """
    insights = []

    # 1. é¢„æµ‹å‡†ç¡®æ€§
    if true_value is not None:
        error = abs(prediction - true_value)
        rel_error = 100 * error / abs(true_value)

        if rel_error < 5:
            insights.append(f"âœ… é¢„æµ‹éå¸¸å‡†ç¡®ï¼ˆç›¸å¯¹è¯¯å·® {rel_error:.2f}%ï¼‰")
        elif rel_error < 15:
            insights.append(f"âš ï¸  é¢„æµ‹è¾ƒå‡†ç¡®ï¼ˆç›¸å¯¹è¯¯å·® {rel_error:.2f}%ï¼‰")
        else:
            insights.append(f"âŒ é¢„æµ‹è¯¯å·®è¾ƒå¤§ï¼ˆç›¸å¯¹è¯¯å·® {rel_error:.2f}%ï¼‰ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")

    # 2. æœ€é‡è¦çš„åŸå­
    top_atoms = atom_df.head(3)
    elements = ', '.join(top_atoms['Element'].tolist())
    avg_importance = top_atoms['Importance'].mean()
    insights.append(f"ğŸ”¬ æœ€é‡è¦çš„åŸå­: {elements}ï¼ˆå¹³å‡é‡è¦æ€§ {avg_importance:.3f}ï¼‰")

    # 3. æœ€é‡è¦çš„åŒ–å­¦é”®
    top_bonds = edge_df.head(3)
    bond_types = ', '.join(top_bonds['bond_type'].tolist())
    insights.append(f"ğŸ”— æœ€é‡è¦çš„åŒ–å­¦é”®: {bond_types}")

    # 4. é…ä½ç¯å¢ƒ
    coord_df = pd.DataFrame(coord_analysis)
    avg_coord = coord_df['coordination_number'].mean()
    most_common_coord = coord_df['coordination_number'].mode()[0]
    insights.append(f"ğŸ”® å¹³å‡é…ä½æ•°: {avg_coord:.2f}ï¼Œæœ€å¸¸è§é…ä½æ•°: {most_common_coord}")

    # 5. é‡è¦å­ç»“æ„
    if substructures:
        top_motif = '-'.join(substructures[0]['elements'])
        insights.append(f"ğŸ§© æœ€é‡è¦çš„å­ç»“æ„åŸºåº: {top_motif}ï¼ˆé‡è¦æ€§ {substructures[0]['total_importance']:.3f}ï¼‰")

    # 6. è·¨æ¨¡æ€æ³¨æ„åŠ›
    if attention_weights is not None:
        if 'graph_to_text' in attention_weights and attention_weights['graph_to_text'] is not None:
            g2t = attention_weights['graph_to_text'].mean().item()
            t2g = attention_weights['text_to_graph'].mean().item()

            if g2t > 0.7:
                insights.append(f"ğŸ’¡ æ¨¡å‹å¼ºçƒˆä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼ˆGraphâ†’Text: {g2t:.3f}ï¼‰")
            elif g2t < 0.3:
                insights.append(f"ğŸ’¡ æ¨¡å‹ä¸»è¦ä¾èµ–å›¾ç»“æ„ä¿¡æ¯ï¼ˆGraphâ†’Text: {g2t:.3f}ï¼‰")
            else:
                insights.append(f"ğŸ’¡ å›¾å’Œæ–‡æœ¬ä¿¡æ¯å‡è¡¡ä½¿ç”¨ï¼ˆGraphâ†’Text: {g2t:.3f}, Textâ†’Graph: {t2g:.3f}ï¼‰")

    # 7. å…ƒç´ å¤šæ ·æ€§
    element_counts = atom_df['Element'].value_counts()
    if len(element_counts) <= 2:
        insights.append(f"âš—ï¸  ç®€å•ç»„æˆï¼ˆ{len(element_counts)} ç§å…ƒç´ ï¼‰: {', '.join(element_counts.index.tolist())}")
    else:
        insights.append(f"âš—ï¸  å¤æ‚ç»„æˆï¼ˆ{len(element_counts)} ç§å…ƒç´ ï¼‰ï¼Œä¸»è¦å…ƒç´ : {', '.join(element_counts.head(3).index.tolist())}")

    # 8. ç»“æ„å¤æ‚åº¦
    edge_per_atom = len(edge_importance) / len(atom_importance)
    if edge_per_atom > 6:
        insights.append(f"ğŸ—ï¸  é«˜è¿æ¥åº¦ç»“æ„ï¼ˆæ¯åŸå­ {edge_per_atom:.1f} æ¡è¾¹ï¼‰")
    elif edge_per_atom < 3:
        insights.append(f"ğŸ—ï¸  ä½è¿æ¥åº¦ç»“æ„ï¼ˆæ¯åŸå­ {edge_per_atom:.1f} æ¡è¾¹ï¼‰")

    return insights


import pandas as pd


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ¯ å®Œæ•´å¯è§£é‡Šæ€§åˆ†ææ¼”ç¤º")
    print("="*80)
    print("\nè¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºè„šæœ¬ã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·ï¼š")
    print("\n1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("2. å‡†å¤‡æµ‹è¯•æ•°æ®")
    print("3. è°ƒç”¨ complete_interpretability_analysis() å‡½æ•°")
    print("\nç¤ºä¾‹ä»£ç :")
    print("-" * 80)
    print("""
from demo_complete_analysis import complete_interpretability_analysis

# åŠ è½½æ¨¡å‹
model = ALIGNN(config.model)
model.load_state_dict(checkpoint['model'])
model.eval()

# è·å–æ ·æœ¬
g, lg, text, label = next(iter(test_loader))
atoms = Atoms.from_dict(sample_data['atoms'])

# æ‰§è¡Œå®Œæ•´åˆ†æ
report = complete_interpretability_analysis(
    model, g, lg, text,
    atoms_object=atoms,
    true_value=label.item(),
    save_dir='./analysis_results',
    sample_id='JVASP-1234'
)

# æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
print(report['files_generated'])
    """)
    print("-" * 80)
    print("\nç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
    print("  âœ… è·¨æ¨¡æ€æ³¨æ„åŠ›çƒ­å›¾")
    print("  âœ… å¤šå¤´æ³¨æ„åŠ›åˆ†æ")
    print("  âœ… åŸå­é‡è¦æ€§å¯è§†åŒ–")
    print("  âœ… è¾¹ï¼ˆåŒ–å­¦é”®ï¼‰é‡è¦æ€§å¯è§†åŒ–")
    print("  âœ… é…ä½ç¯å¢ƒåˆ†æ")
    print("  âœ… é‡è¦å­ç»“æ„è¯†åˆ«")
    print("  âœ… ç»¼åˆåˆ†ææ–‡æœ¬æŠ¥å‘Š")
    print("\n" + "="*80 + "\n")


if __name__ == '__main__':
    demo()
