#!/usr/bin/env python
"""
ä»è®­ç»ƒè¾“å‡ºç›®å½•ç›´æ¥è¿›è¡Œæ½œåœ¨ç©ºé—´å¯è§†åŒ–
æ— éœ€æ‰‹åŠ¨æŒ‡å®šdata loaderï¼Œè‡ªåŠ¨ä»è¾“å‡ºç›®å½•é‡å»ºæ•°æ®é›†
"""

import os
import sys
import argparse
import torch
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from data import get_train_val_loaders
from config import TrainingConfig
from models.alignn import ALIGNN, ALIGNNConfig
import visualize_latent_space as vis


def load_training_config(output_dir):
    """ä»è¾“å‡ºç›®å½•åŠ è½½è®­ç»ƒé…ç½®"""
    # å°è¯•ä»historyæ–‡ä»¶ä¸­è·å–é…ç½®ä¿¡æ¯
    import json

    # æ£€æŸ¥æ˜¯å¦æœ‰config.json
    config_file = os.path.join(output_dir, 'config.json')
    if os.path.exists(config_file):
        with open(config_file, 'r') as f:
            config_dict = json.load(f)
        return config_dict

    # å¦‚æœæ²¡æœ‰ï¼Œæç¤ºç”¨æˆ·éœ€è¦æ‰‹åŠ¨æŒ‡å®šå‚æ•°
    return None


def main():
    parser = argparse.ArgumentParser(description='ä»è®­ç»ƒè¾“å‡ºç›®å½•è¿›è¡Œæ½œåœ¨ç©ºé—´å¯è§†åŒ–')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--output_dir', type=str, required=True,
                        help='è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆåŒ…å«best_val_model.ptç­‰ï¼‰')

    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--dataset', type=str, required=True,
                        choices=['jarvis', 'mp', 'class'],
                        help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--property', type=str, required=True,
                        help='å±æ€§åç§°ï¼ˆå¦‚ formation_energy, synç­‰ï¼‰')
    parser.add_argument('--root_dir', type=str, default='./dataset',
                        help='æ•°æ®é›†æ ¹ç›®å½•')

    # å¯é€‰å‚æ•°
    parser.add_argument('--checkpoint', type=str, default='best_val_model.pt',
                        help='checkpointæ–‡ä»¶åï¼ˆåœ¨output_dirä¸­ï¼‰')
    parser.add_argument('--split', type=str, default='test',
                        choices=['train', 'val', 'test'],
                        help='ä½¿ç”¨å“ªä¸ªæ•°æ®é›†split')
    parser.add_argument('--save_dir', type=str, default=None,
                        help='ä¿å­˜å›¾ç‰‡çš„ç›®å½•ï¼ˆé»˜è®¤ä¸ºoutput_dir/latent_space_visï¼‰')
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¾å¤‡ (cpu æˆ– cuda)')
    parser.add_argument('--feature_types', nargs='+',
                        default=['graph', 'text', 'fused'],
                        choices=['graph', 'text', 'fused'],
                        help='è¦å¯è§†åŒ–çš„ç‰¹å¾ç±»å‹')
    parser.add_argument('--methods', nargs='+', default=['tsne'],
                        choices=['tsne', 'umap'],
                        help='é™ç»´æ–¹æ³•ï¼ˆé»˜è®¤åªç”¨tsneï¼Œå› ä¸ºumapéœ€è¦é¢å¤–å®‰è£…ï¼‰')
    parser.add_argument('--dimensions', nargs='+', type=int, default=[2],
                        choices=[2, 3],
                        help='é™ç»´ç»´åº¦ï¼ˆé»˜è®¤åª2Dï¼Œ3Då¯èƒ½è¾ƒæ…¢ï¼‰')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¤§æ•°æ®é›†ï¼ŒåŠ é€Ÿå¯è§†åŒ–ï¼‰')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½workeræ•°')

    args = parser.parse_args()

    # è®¾ç½®ä¿å­˜ç›®å½•
    if args.save_dir is None:
        args.save_dir = os.path.join(args.output_dir, 'latent_space_vis')

    # æ£€æŸ¥checkpoint
    checkpoint_path = os.path.join(args.output_dir, args.checkpoint)
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æœªæ‰¾åˆ°checkpoint: {checkpoint_path}")
        return

    print("="*70)
    print("ğŸ¨ æ½œåœ¨ç©ºé—´å¯è§†åŒ–ï¼ˆä»è¾“å‡ºç›®å½•ï¼‰")
    print("="*70)
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"Checkpoint: {args.checkpoint}")
    print(f"æ•°æ®é›†: {args.dataset}/{args.property}")
    print(f"Split: {args.split}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print()

    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    model_config = checkpoint.get('config', None)

    if model_config is None:
        print("âŒ Checkpointä¸­æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œæ— æ³•é‡å»ºæ¨¡å‹")
        return

    model = ALIGNN(model_config)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    # æ£€æµ‹æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
    is_classification = model_config.classification if hasattr(model_config, 'classification') else False
    print(f"ä»»åŠ¡ç±»å‹: {'åˆ†ç±»' if is_classification else 'å›å½’'}")

    # 2. é‡å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š é‡å»ºæ•°æ®åŠ è½½å™¨...")

    try:
        # å¯¼å…¥æ•°æ®åŠ è½½å‡½æ•°
        from train_with_cross_modal_attention import load_dataset, get_dataset_paths

        # æ ¹æ®æ•°æ®é›†ç±»å‹ç¡®å®šè·¯å¾„
        dataset_mapping = {
            'jarvis': 'jarvis',
            'mp': 'mp',
            'class': 'class'
        }

        actual_dataset = dataset_mapping.get(args.dataset.lower(), args.dataset.lower())

        # è·å–æ•°æ®é›†è·¯å¾„
        cif_dir, id_prop_file = get_dataset_paths(args.root_dir, actual_dataset, args.property)

        # åŠ è½½æ•°æ®é›†
        df = load_dataset(cif_dir, id_prop_file, actual_dataset, args.property)
        print(f"âœ… åŠ è½½æ•°æ®é›†: {len(df)} æ ·æœ¬")

        # å¦‚æœè®¾ç½®äº†max_samplesï¼Œè¿›è¡Œé‡‡æ ·
        if args.max_samples and len(df) > args.max_samples:
            print(f"âš ï¸  æ•°æ®é›†è¿‡å¤§ï¼Œéšæœºé‡‡æ · {args.max_samples} æ ·æœ¬ç”¨äºå¯è§†åŒ–")
            import random
            random.seed(42)
            df = random.sample(df, args.max_samples)

        # ç›´æ¥åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä¸ä½¿ç”¨TrainingConfigï¼ˆé¿å…dataseté™åˆ¶ï¼‰
        (train_loader, val_loader, test_loader,
         prepare_batch) = get_train_val_loaders(
            dataset='user_data',  # ä½¿ç”¨user_dataé¿å…dataseté™åˆ¶
            dataset_array=df,
            target='target',
            n_train=None,
            n_val=None,
            n_test=None,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            batch_size=args.batch_size,
            atom_features=model_config.atom_features if hasattr(model_config, 'atom_features') else 'cgcnn',
            neighbor_strategy='k-nearest',
            line_graph=model_config.line_graph if hasattr(model_config, 'line_graph') else True,
            split_seed=42,
            workers=args.num_workers,
            pin_memory=False,
            save_dataloader=False,
            filename='temp_vis',
            id_tag='jid',
            use_canonize=True,
            cutoff=8.0,
            max_neighbors=12,
            output_dir=args.output_dir,
        )

        # è·å–æ•°æ®é›†å¯¹è±¡
        train_data = train_loader.dataset
        val_data = val_loader.dataset
        test_data = test_loader.dataset

        # é€‰æ‹©è¦å¯è§†åŒ–çš„split
        if args.split == 'train':
            data_loader = train_loader
            print(f"âœ… ä½¿ç”¨è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        elif args.split == 'val':
            data_loader = val_loader
            print(f"âœ… ä½¿ç”¨éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        else:
            data_loader = test_loader
            print(f"âœ… ä½¿ç”¨æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")

    except Exception as e:
        print(f"âŒ é‡å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # 3. è¿è¡Œå¯è§†åŒ–
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = 'cpu'

    vis.visualize_latent_space(
        checkpoint_path=checkpoint_path,
        data_loader=data_loader,
        save_dir=args.save_dir,
        device=device,
        feature_types=args.feature_types,
        methods=args.methods,
        dimensions=args.dimensions,
        is_classification=is_classification
    )


if __name__ == '__main__':
    main()
