# 注意力权重对比分析指南

## 📋 分析目标

通过对比**全模态模型**（中期+细粒度+全局）和**无中期融合模型**（细粒度+全局）的注意力权重，验证中期融合如何改善节点-文本对齐。

## 🎯 核心假设

```
假设：中期融合改善了节点特征 → 细粒度注意力对齐更精准

全模态（有中期融合）:
├─ 节点已包含文本信息（通过中期融合）
├─ 细粒度注意力在更好的基础上对齐
└─ 预期：注意力更集中、更准确

无中期融合:
├─ 节点和文本从未交互
├─ 细粒度注意力需要从零学习对齐
└─ 预期：注意力更分散、更模糊
```

## 📊 分析指标

| 指标 | 含义 | 期望（全模态） | 解释 |
|------|------|----------------|------|
| **注意力熵** | 分布的不确定性 | **更低** ↓ | 注意力更集中在关键token上 |
| **最大权重** | 最强对齐的权重值 | **更高** ↑ | 模型更确信重要的对齐 |
| **有效Token数** | 权重>0.1的token数 | **更少** ↓ | 更有选择性，不分散注意力 |
| **基尼系数** | 权重分布的不平等度 | **更高** ↑ | 少数token获得大部分注意力 |

### 指标详解

#### 1. 注意力熵（Entropy）
```
熵 = -Σ p_i * log(p_i)

低熵 → 注意力集中在少数token
高熵 → 注意力分散在多个token

示例：
集中分布: [0.8, 0.1, 0.05, 0.05] → 熵 ≈ 0.92
分散分布: [0.25, 0.25, 0.25, 0.25] → 熵 ≈ 1.39
```

#### 2. 最大权重（Max Weight）
```
max_weight = max(attention_weights)

高值 → 模型明确知道哪个token最重要
低值 → 模型不确定，注意力平均分配
```

#### 3. 有效Token数（Effective Tokens）
```
effective_tokens = count(weight > threshold)

少 → 只关注关键token（高效）
多 → 关注很多token（可能无关）
```

#### 4. 基尼系数（Gini Coefficient）
```
基尼系数 ∈ [0, 1]

高 → 不平等（少数token主导）
低 → 平等（所有token权重相近）
```

## 🚀 使用方法

### 步骤 1：准备模型

你需要两个训练好的模型：

```bash
# 全模态模型（中期+细粒度+全局）
checkpoint_full = "outputs/exp_full_model/best_model.pt"

# 无中期融合模型（细粒度+全局）
checkpoint_no_middle = "outputs/exp_no_middle/best_model.pt"
```

### 步骤 2：运行分析

```bash
python compare_attention_weights.py \
    --checkpoint_full outputs/exp_full_model/best_model.pt \
    --checkpoint_no_middle outputs/exp_no_middle/best_model.pt \
    --root_dir /path/to/your/data \
    --save_dir ./attention_comparison \
    --num_samples 100 \
    --batch_size 32
```

### 步骤 3：查看结果

分析会生成：

```
attention_comparison/
├── attention_statistics_comparison.png  # 统计指标对比图
├── attention_statistics.csv             # 详细统计数据
├── attention_heatmap_example_1.png      # 示例1的热图对比
├── attention_heatmap_example_2.png      # 示例2的热图对比
└── attention_heatmap_example_3.png      # 示例3的热图对比
```

## 📈 预期结果

如果中期融合确实改善了对齐，应该看到：

| 指标 | 全模态 | 无中期 | 差异 | 含义 |
|------|--------|--------|------|------|
| 注意力熵 | ↓ 更低 | ↑ 更高 | -X% | 注意力更集中 |
| 最大权重 | ↑ 更高 | ↓ 更低 | +X% | 对齐更明确 |
| 有效Token数 | ↓ 更少 | ↑ 更多 | -X% | 更有选择性 |
| 基尼系数 | ↑ 更高 | ↓ 更低 | +X% | 注意力更集中于关键token |

## 🔍 结果解读

### 场景 1：假设验证（理想情况）

```
结果：
├─ 注意力熵：0.85 (全模态) vs 1.12 (无中期) ✅ 降低24%
├─ 最大权重：0.45 (全模态) vs 0.32 (无中期) ✅ 提升41%
├─ 有效Token数：3.2 (全模态) vs 5.8 (无中期) ✅ 减少45%
└─ 基尼系数：0.68 (全模态) vs 0.52 (无中期) ✅ 提升31%

解释：
中期融合显著改善了细粒度注意力的对齐质量。节点在中期融合后
已经"感知"到文本信息，因此细粒度注意力能更精准地找到关键的
节点-token对应关系。这解释了为什么全模态模型的MAE更低。
```

### 场景 2：假设部分支持

```
结果：
├─ 注意力熵：0.92 (全模态) vs 0.95 (无中期) ~ 微降3%
├─ 最大权重：0.38 (全模态) vs 0.35 (无中期) ~ 微升9%
└─ 但MAE仍然：0.255 vs 0.274 (显著提升)

解释：
中期融合的作用可能不仅仅是改善注意力对齐，还包括：
- 增加特征多样性
- 提升泛化能力
- 改善特征的非线性表达能力
注意力权重的差异不明显，但整体性能提升显著。
```

### 场景 3：意外发现

```
结果：
├─ 注意力熵：全模态 > 无中期（反直觉）
└─ 但MAE：全模态 < 无中期（性能更好）

可能解释：
- 全模态模型学到了更复杂的对齐策略
- 不是简单地集中注意力，而是学习多样化的对齐模式
- 这种复杂性虽然提高了熵，但提升了泛化能力
```

## 🎨 可视化示例解读

### 注意力热图（Heatmap）

```
示例热图：
        Token 1  Token 2  Token 3  Token 4
Node 1   0.1      0.7      0.1      0.1    ← 集中在Token 2
Node 2   0.2      0.2      0.5      0.1    ← 集中在Token 3
Node 3   0.3      0.1      0.1      0.5    ← 集中在Token 4

全模态：颜色更鲜明、对比度更高 → 对齐更明确
无中期：颜色更均匀、对比度较低 → 对齐较模糊
```

## 📝 论文写作建议

### 标题
"中期融合改善节点-文本对齐：基于注意力权重的实证分析"

### 核心论述

```markdown
为了验证中期融合改善节点-文本对齐的机制，我们对比了全模态
模型和无中期融合模型的细粒度注意力权重。

结果显示，全模态模型的注意力分布显著更集中（熵降低X%），
最大权重更高（提升Y%），有效token数更少（减少Z%）。这表明
中期融合确实改善了节点特征的质量，使得细粒度注意力能够更
精准地识别节点-token的对应关系。

如图X所示，全模态模型的注意力热图呈现出更明确的块状结构，
而无中期融合模型的注意力较为分散。这一差异直接解释了为什么
全模态模型的预测性能更优（MAE 0.255 vs 0.274）。
```

## 🔧 扩展分析

### 可选分析 1：按层分析

如果模型有多层细粒度注意力，可以对比各层的权重变化。

### 可选分析 2：按样本类型分析

对比简单样本和复杂样本的注意力差异。

### 可选分析 3：注意力-性能相关性

分析注意力熵与预测误差的相关性。

## 📚 参考文献建议

- Attention is All You Need (Vaswani et al., 2017)
- Visualizing Attention in Transformer Models
- Analyzing Multi-Head Self-Attention

## ⚠️ 注意事项

1. **样本数量**：建议至少100个样本保证统计显著性
2. **批处理**：注意GPU内存，适当调整batch_size
3. **随机性**：使用固定的random_seed保证可复现性
4. **数据分布**：确保两个模型在相同的测试集上对比

## 🎯 下一步

分析完成后，可以：
1. 将结果写入论文的消融研究部分
2. 在补充材料中提供更多可视化
3. 进行统计显著性检验（t-test）
4. 分析特定样本的case study
