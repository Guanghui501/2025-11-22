#!/usr/bin/env python
"""
å¯¹æ¯”ä¸åŒèåˆæœºåˆ¶çš„æ•ˆæœ
é€šè¿‡æ¶ˆèå®éªŒç›´è§‚å±•ç¤ºå„ä¸ªæ¨¡å—çš„ä½œç”¨
ç‰ˆæœ¬2: ä½¿ç”¨return_intermediate_featureså‚æ•°ï¼Œé¿å…åŠ¨æ€ä¿®æ”¹æ¨¡å‹æ¶æ„
"""

import os
import sys
import argparse
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

from models.alignn import ALIGNN, ALIGNNConfig
from data import get_train_val_loaders

sns.set_style("whitegrid")
plt.rcParams['font.size'] = 10


class FusionComparator:
    """èåˆæœºåˆ¶å¯¹æ¯”å™¨"""

    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

    def extract_features_ablation(self, data_loader, max_samples=None):
        """
        æå–ä¸åŒé˜¶æ®µçš„ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰

        Returns:
            features_dict: {
                'graph_base': å›¾åŸºç¡€ç‰¹å¾ï¼ˆæŠ•å½±åï¼Œèåˆå‰ï¼‰,
                'text_base': æ–‡æœ¬åŸºç¡€ç‰¹å¾ï¼ˆæŠ•å½±åï¼Œèåˆå‰ï¼‰,
                'graph_cross': åº”ç”¨å…¨å±€æ³¨æ„åŠ›åçš„å›¾ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰,
                'text_cross': åº”ç”¨å…¨å±€æ³¨æ„åŠ›åçš„æ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰,
                'graph_final': æœ€ç»ˆå›¾ç‰¹å¾,
                'text_final': æœ€ç»ˆæ–‡æœ¬ç‰¹å¾,
                'fused': æœ€ç»ˆèåˆç‰¹å¾
            }
            targets: ç›®æ ‡å€¼
            ids: æ ·æœ¬ID
        """
        print("ğŸ”„ æå–ä¸åŒé˜¶æ®µçš„ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰...")

        # æ£€æŸ¥æ¨¡å‹é…ç½®
        has_middle = self.model.use_middle_fusion
        has_fine = self.model.use_fine_grained_attention
        has_cross = self.model.use_cross_modal_attention

        print(f"   æ¨¡å‹é…ç½®: ä¸­é—´èåˆ={has_middle}, ç»†ç²’åº¦æ³¨æ„åŠ›={has_fine}, å…¨å±€æ³¨æ„åŠ›={has_cross}")

        features_dict = {
            'graph_base': [],
            'text_base': [],
            'graph_cross': [],
            'text_cross': [],
            'graph_final': [],
            'text_final': [],
            'fused': []
        }
        targets = []
        ids = []

        sample_count = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(data_loader, desc="æå–ç‰¹å¾")):
                if len(batch) == 3:
                    g, text, target = batch
                    lg = None
                elif len(batch) == 4:
                    g, lg, text, target = batch
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„batchæ ¼å¼: {len(batch)}ä¸ªå…ƒç´ ")

                g = g.to(self.device)
                if lg is not None:
                    lg = lg.to(self.device)

                # å¤„ç†text
                if isinstance(text, dict):
                    text = {k: v.to(self.device) for k, v in text.items()}
                elif isinstance(text, (list, tuple)):
                    # textæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¿æŒä¸åŠ¨
                    pass
                elif torch.is_tensor(text):
                    text = text.to(self.device)

                batch_size = target.size(0)

                # æ„å»ºæ¨¡å‹è¾“å…¥
                if lg is not None:
                    model_input = (g, lg, text)
                else:
                    model_input = (g, text)

                # æå–ä¸­é—´ç‰¹å¾ï¼ˆä½¿ç”¨æ–°çš„return_intermediate_featureså‚æ•°ï¼‰
                output = self.model(model_input, return_intermediate_features=True)

                # åŸºç¡€ç‰¹å¾ï¼ˆèåˆå‰ï¼‰
                features_dict['graph_base'].append(output['graph_base'].cpu().numpy())
                features_dict['text_base'].append(output['text_base'].cpu().numpy())

                # æœ€ç»ˆç‰¹å¾
                features_dict['graph_final'].append(output['graph_features'].cpu().numpy())
                features_dict['text_final'].append(output['text_features'].cpu().numpy())

                # å…¨å±€æ³¨æ„åŠ›åçš„ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if has_cross and 'graph_cross' in output:
                    features_dict['graph_cross'].append(output['graph_cross'].cpu().numpy())
                    features_dict['text_cross'].append(output['text_cross'].cpu().numpy())

                # èåˆç‰¹å¾
                fused = np.concatenate([
                    output['graph_features'].cpu().numpy(),
                    output['text_features'].cpu().numpy()
                ], axis=1)
                features_dict['fused'].append(fused)

                targets.append(target.cpu().numpy())

                # è®°å½•æ ·æœ¬IDï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(g, 'ndata') and 'jid' in g.ndata:
                    batch_ids = [g.ndata['jid'][i] for i in range(g.batch_size)]
                    ids.extend(batch_ids)

                sample_count += batch_size
                if max_samples and sample_count >= max_samples:
                    break

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        for key in features_dict:
            if len(features_dict[key]) > 0:
                features_dict[key] = np.concatenate(features_dict[key], axis=0)
            else:
                features_dict[key] = None

        targets = np.concatenate(targets, axis=0)

        # ç§»é™¤ç©ºç‰¹å¾
        features_dict = {k: v for k, v in features_dict.items() if v is not None}

        print(f"âœ… æå–å®Œæˆ! æ ·æœ¬æ•°: {len(targets)}, ç‰¹å¾ç±»å‹: {list(features_dict.keys())}")

        return features_dict, targets, ids

    def visualize_tsne(self, features_dict, targets, save_dir):
        """ä½¿ç”¨t-SNEå¯è§†åŒ–ä¸åŒé˜¶æ®µçš„ç‰¹å¾"""
        print("\nğŸ“Š ç”Ÿæˆt-SNEå¯è§†åŒ–...")

        # ç¡®å®šè¦å¯è§†åŒ–çš„ç‰¹å¾
        feature_names = []
        feature_data = []

        if 'graph_base' in features_dict:
            feature_names.append('Graph Base')
            feature_data.append(features_dict['graph_base'])

        if 'text_base' in features_dict:
            feature_names.append('Text Base')
            feature_data.append(features_dict['text_base'])

        if 'graph_cross' in features_dict:
            feature_names.append('Graph + Cross-Modal')
            feature_data.append(features_dict['graph_cross'])

        if 'text_cross' in features_dict:
            feature_names.append('Text + Cross-Modal')
            feature_data.append(features_dict['text_cross'])

        if 'graph_final' in features_dict:
            feature_names.append('Graph Final')
            feature_data.append(features_dict['graph_final'])

        if 'text_final' in features_dict:
            feature_names.append('Text Final')
            feature_data.append(features_dict['text_final'])

        if 'fused' in features_dict:
            feature_names.append('Fused')
            feature_data.append(features_dict['fused'])

        n_features = len(feature_names)
        if n_features == 0:
            print("âš ï¸  æ²¡æœ‰å¯è§†åŒ–çš„ç‰¹å¾!")
            return

        # åˆ›å»ºç½‘æ ¼å¸ƒå±€
        n_cols = min(3, n_features)
        n_rows = (n_features + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
        if n_features == 1:
            axes = [axes]
        else:
            axes = axes.flatten() if n_features > 1 else [axes]

        # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œt-SNE
        for idx, (name, features) in enumerate(zip(feature_names, feature_data)):
            print(f"   å¤„ç† {name}...")
            tsne = TSNE(n_components=2, random_state=42, perplexity=30)
            features_2d = tsne.fit_transform(features)

            ax = axes[idx]
            scatter = ax.scatter(features_2d[:, 0], features_2d[:, 1],
                                c=targets, cmap='viridis', alpha=0.6, s=20)
            ax.set_title(name, fontsize=12, fontweight='bold')
            ax.set_xlabel('t-SNE 1')
            ax.set_ylabel('t-SNE 2')
            plt.colorbar(scatter, ax=ax, label='Target Value')

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_features, len(axes)):
            axes[idx].axis('off')

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'tsne_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… t-SNEå¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        plt.close()

    def compute_metrics(self, features_dict, targets, save_dir):
        """è®¡ç®—ä¸åŒç‰¹å¾çš„è´¨é‡æŒ‡æ ‡"""
        print("\nğŸ“ˆ è®¡ç®—ç‰¹å¾è´¨é‡æŒ‡æ ‡...")

        metrics_list = []

        for name, features in features_dict.items():
            if features is None or len(features) == 0:
                continue

            print(f"   åˆ†æ {name}...")

            # Silhouette Score (è½®å»“ç³»æ•°, è¶Šå¤§è¶Šå¥½)
            try:
                sil_score = silhouette_score(features, targets)
            except:
                sil_score = np.nan

            # Davies-Bouldin Index (è¶Šå°è¶Šå¥½)
            try:
                db_score = davies_bouldin_score(features, targets)
            except:
                db_score = np.nan

            # Intra-class similarity (ç±»å†…ç›¸ä¼¼åº¦, è¶Šå¤§è¶Šå¥½)
            intra_sim = self._compute_intra_class_similarity(features, targets)

            # Inter-class separation (ç±»é—´åˆ†ç¦»åº¦, è¶Šå¤§è¶Šå¥½)
            inter_sep = self._compute_inter_class_separation(features, targets)

            metrics_list.append({
                'Feature': name,
                'Silhouette Score': sil_score,
                'Davies-Bouldin Index': db_score,
                'Intra-class Similarity': intra_sim,
                'Inter-class Separation': inter_sep
            })

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(metrics_list)
        save_path = os.path.join(save_dir, 'feature_metrics.csv')
        df.to_csv(save_path, index=False)
        print(f"\nâœ… æŒ‡æ ‡å·²ä¿å­˜: {save_path}")
        print("\n" + df.to_string(index=False))

        # å¯è§†åŒ–æŒ‡æ ‡
        self._plot_metrics(df, save_dir)

        return df

    def _compute_intra_class_similarity(self, features, targets):
        """è®¡ç®—ç±»å†…ç›¸ä¼¼åº¦"""
        unique_targets = np.unique(targets)
        if len(unique_targets) < 2:
            return 1.0

        sims = []
        for target in unique_targets[:10]:  # åªå–å‰10ä¸ªç±»åˆ«é¿å…è®¡ç®—è¿‡æ…¢
            mask = targets == target
            if np.sum(mask) < 2:
                continue
            class_features = features[mask]
            sim_matrix = cosine_similarity(class_features)
            # å–ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
            upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]
            sims.append(np.mean(upper_tri))

        return np.mean(sims) if len(sims) > 0 else 0.0

    def _compute_inter_class_separation(self, features, targets):
        """è®¡ç®—ç±»é—´åˆ†ç¦»åº¦"""
        unique_targets = np.unique(targets)
        if len(unique_targets) < 2:
            return 0.0

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ä¸­å¿ƒ
        centroids = []
        for target in unique_targets[:10]:  # åªå–å‰10ä¸ªç±»åˆ«
            mask = targets == target
            if np.sum(mask) == 0:
                continue
            centroids.append(np.mean(features[mask], axis=0))

        if len(centroids) < 2:
            return 0.0

        centroids = np.array(centroids)
        # è®¡ç®—ä¸­å¿ƒä¹‹é—´çš„å¹³å‡è·ç¦»
        distances = []
        for i in range(len(centroids)):
            for j in range(i+1, len(centroids)):
                dist = np.linalg.norm(centroids[i] - centroids[j])
                distances.append(dist)

        return np.mean(distances)

    def _plot_metrics(self, df, save_dir):
        """å¯è§†åŒ–æŒ‡æ ‡å¯¹æ¯”"""
        metrics = ['Silhouette Score', 'Davies-Bouldin Index',
                   'Intra-class Similarity', 'Inter-class Separation']

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            data = df[['Feature', metric]].dropna()

            if len(data) == 0:
                continue

            x = range(len(data))
            y = data[metric].values
            labels = data['Feature'].values

            bars = ax.bar(x, y, alpha=0.7, color=sns.color_palette("husl", len(data)))
            ax.set_xticks(x)
            ax.set_xticklabels(labels, rotation=45, ha='right')
            ax.set_ylabel(metric)
            ax.set_title(f'{metric} Comparison', fontweight='bold')
            ax.grid(axis='y', alpha=0.3)

            # æ ‡æ³¨æ•°å€¼
            for i, v in enumerate(y):
                ax.text(i, v + 0.01*max(y), f'{v:.3f}', ha='center', va='bottom', fontsize=9)

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'metrics_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        plt.close()


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”ä¸åŒèåˆæœºåˆ¶çš„æ•ˆæœ (v2)')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, default='dft_3d',
                        choices=['dft_3d', 'dft_2d', 'megnet', 'cfid_3d', 'qm9_std_jctc'],
                        help='JARVISæ•°æ®é›†åç§° (é»˜è®¤: dft_3d)')
    parser.add_argument('--property', type=str, default='formation_energy_peratom',
                        help='ç›®æ ‡å±æ€§')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_samples', type=int, default=500, help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--save_dir', type=str, default='./fusion_comparison',
                        help='ç»“æœä¿å­˜ç›®å½•')
    args = parser.parse_args()

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location='cpu', weights_only=False)

    if 'config' in checkpoint:
        config = checkpoint['config']
    else:
        raise ValueError("Checkpointä¸­æ²¡æœ‰æ‰¾åˆ°config")

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = ALIGNN(config)
    model.load_state_dict(checkpoint['model'])
    model = model.to(device)
    model.eval()

    print(f"   æ¨¡å‹é…ç½®:")
    print(f"     - ä¸­é—´èåˆ: {model.use_middle_fusion}")
    print(f"     - ç»†ç²’åº¦æ³¨æ„åŠ›: {model.use_fine_grained_attention}")
    print(f"     - å…¨å±€æ³¨æ„åŠ›: {model.use_cross_modal_attention}")

    # åŠ è½½æ•°æ®
    print(f"\nğŸ”„ åŠ è½½æ•°æ®é›†: {args.dataset} - {args.property}")
    train_loader, val_loader, test_loader = get_train_val_loaders(
        dataset=args.dataset,
        target=args.property,
        n_train=None,
        n_val=None,
        n_test=None,
        batch_size=args.batch_size,
        workers=0,
        output_dir=args.save_dir
    )

    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = FusionComparator(model, device=device)

    # æå–ç‰¹å¾
    features_dict, targets, ids = comparator.extract_features_ablation(
        test_loader, max_samples=args.max_samples
    )

    # å¯è§†åŒ–
    comparator.visualize_tsne(features_dict, targets, args.save_dir)

    # è®¡ç®—æŒ‡æ ‡
    metrics_df = comparator.compute_metrics(features_dict, targets, args.save_dir)

    print(f"\nğŸ‰ åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.save_dir}")


if __name__ == '__main__':
    main()
