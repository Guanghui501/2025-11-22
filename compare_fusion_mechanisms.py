#!/usr/bin/env python
"""
å¯¹æ¯”ä¸åŒèåˆæœºåˆ¶çš„æ•ˆæœ
é€šè¿‡æ¶ˆèå®éªŒç›´è§‚å±•ç¤ºå„ä¸ªæ¨¡å—çš„ä½œç”¨
"""

import os
import sys
import argparse
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

from models.alignn import ALIGNN, ALIGNNConfig
from data import get_train_val_loaders

sns.set_style("whitegrid")
plt.rcParams['font.size'] = 10


class FusionComparator:
    """èåˆæœºåˆ¶å¯¹æ¯”å™¨"""

    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

    def extract_features_ablation(self, data_loader, max_samples=None):
        """
        æå–ä¸åŒé˜¶æ®µçš„ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰

        Returns:
            features_dict: {
                'graph_base': å›¾åŸºç¡€ç‰¹å¾ï¼ˆæ— ä»»ä½•èåˆï¼‰,
                'text_base': æ–‡æœ¬åŸºç¡€ç‰¹å¾ï¼ˆæ— ä»»ä½•èåˆï¼‰,
                'graph_middle': åº”ç”¨ä¸­é—´èåˆåçš„å›¾ç‰¹å¾,
                'graph_fine': åº”ç”¨ç»†ç²’åº¦æ³¨æ„åŠ›åçš„å›¾ç‰¹å¾,
                'text_fine': åº”ç”¨ç»†ç²’åº¦æ³¨æ„åŠ›åçš„æ–‡æœ¬ç‰¹å¾,
                'graph_cross': åº”ç”¨å…¨å±€æ³¨æ„åŠ›åçš„å›¾ç‰¹å¾,
                'text_cross': åº”ç”¨å…¨å±€æ³¨æ„åŠ›åçš„æ–‡æœ¬ç‰¹å¾,
                'fused': æœ€ç»ˆèåˆç‰¹å¾
            }
            targets: ç›®æ ‡å€¼
            ids: æ ·æœ¬ID
        """
        print("ğŸ”„ æå–ä¸åŒé˜¶æ®µçš„ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰...")

        # ä¿å­˜åŸå§‹é…ç½®
        original_middle = self.model.use_middle_fusion
        original_fine = self.model.use_fine_grained_attention
        original_cross = self.model.use_cross_modal_attention

        features_dict = {
            'graph_base': [],
            'text_base': [],
            'graph_middle': [],
            'graph_fine': [],
            'text_fine': [],
            'graph_cross': [],
            'text_cross': [],
            'fused': []
        }
        targets = []
        ids = []

        sample_count = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(data_loader, desc="æå–ç‰¹å¾")):
                if len(batch) == 3:
                    g, text, target = batch
                    lg = None
                elif len(batch) == 4:
                    g, lg, text, target = batch
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„batchæ ¼å¼")

                g = g.to(self.device)
                if lg is not None:
                    lg = lg.to(self.device)

                # å¤„ç†text
                if isinstance(text, dict):
                    text = {k: v.to(self.device) for k, v in text.items()}
                elif isinstance(text, (list, tuple)):
                    # textæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¿æŒä¸åŠ¨
                    pass
                elif torch.is_tensor(text):
                    text = text.to(self.device)

                batch_size = target.size(0)

                # æ„å»ºæ¨¡å‹è¾“å…¥
                if lg is not None:
                    model_input = (g, lg, text)
                else:
                    model_input = (g, text)

                # ========== 1. åŸºç¡€ç‰¹å¾ï¼ˆå…³é—­æ‰€æœ‰èåˆï¼‰==========
                self.model.use_middle_fusion = False
                self.model.use_fine_grained_attention = False
                self.model.use_cross_modal_attention = False

                output_base = self.model(model_input, return_features=True)
                features_dict['graph_base'].append(output_base['graph_features'].cpu().numpy())
                features_dict['text_base'].append(output_base['text_features'].cpu().numpy())

                # ========== 2. ä¸­é—´èåˆç‰¹å¾ ==========
                if original_middle:
                    self.model.use_middle_fusion = True
                    self.model.use_fine_grained_attention = False
                    self.model.use_cross_modal_attention = False

                    output_middle = self.model(model_input, return_features=True)
                    features_dict['graph_middle'].append(output_middle['graph_features'].cpu().numpy())

                # ========== 3. ç»†ç²’åº¦æ³¨æ„åŠ›ç‰¹å¾ ==========
                if original_fine:
                    self.model.use_middle_fusion = original_middle  # ä¿ç•™ä¸­é—´èåˆ
                    self.model.use_fine_grained_attention = True
                    self.model.use_cross_modal_attention = False

                    output_fine = self.model(model_input, return_features=True)
                    features_dict['graph_fine'].append(output_fine['graph_features'].cpu().numpy())
                    features_dict['text_fine'].append(output_fine['text_features'].cpu().numpy())

                # ========== 4. å…¨å±€æ³¨æ„åŠ›ç‰¹å¾ ==========
                if original_cross:
                    self.model.use_middle_fusion = original_middle
                    self.model.use_fine_grained_attention = original_fine
                    self.model.use_cross_modal_attention = True

                    output_cross = self.model(model_input, return_features=True)
                    features_dict['graph_cross'].append(output_cross['graph_features'].cpu().numpy())
                    features_dict['text_cross'].append(output_cross['text_features'].cpu().numpy())

                    # èåˆç‰¹å¾
                    fused = np.concatenate([
                        output_cross['graph_features'].cpu().numpy(),
                        output_cross['text_features'].cpu().numpy()
                    ], axis=1)
                    features_dict['fused'].append(fused)

                targets.append(target.cpu().numpy())

                # è®°å½•æ ·æœ¬IDï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(g, 'ndata') and 'jid' in g.ndata:
                    batch_ids = [g.ndata['jid'][i] for i in range(batch_size)]
                    ids.extend(batch_ids)

                sample_count += batch_size
                if max_samples and sample_count >= max_samples:
                    break

        # æ¢å¤åŸå§‹é…ç½®
        self.model.use_middle_fusion = original_middle
        self.model.use_fine_grained_attention = original_fine
        self.model.use_cross_modal_attention = original_cross

        # åˆå¹¶æ‰€æœ‰batch
        for key in features_dict:
            if features_dict[key]:
                features_dict[key] = np.vstack(features_dict[key])
            else:
                features_dict[key] = None

        targets = np.concatenate(targets)

        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå…± {len(targets)} ä¸ªæ ·æœ¬")
        for key, feat in features_dict.items():
            if feat is not None:
                print(f"   {key}: {feat.shape}")

        return features_dict, targets, ids


def visualize_feature_comparison(features_dict, targets, save_dir, method='tsne',
                                 is_classification=False):
    """
    å¯è§†åŒ–ä¸åŒèåˆç­–ç•¥çš„ç‰¹å¾ç©ºé—´
    """
    print(f"\nğŸ¨ ä½¿ç”¨{method.upper()}è¿›è¡Œç‰¹å¾å¯¹æ¯”å¯è§†åŒ–...")

    # å®šä¹‰è¦å¯¹æ¯”çš„ç‰¹å¾ç»„åˆ
    comparisons = [
        ('graph_base', 'Graph (No Fusion)'),
        ('text_base', 'Text (No Fusion)'),
        ('graph_middle', 'Graph (+ Middle Fusion)'),
        ('graph_fine', 'Graph (+ Fine-grained Attn)'),
        ('graph_cross', 'Graph (+ Cross-modal Attn)'),
        ('fused', 'Fused (All Mechanisms)')
    ]

    # è¿‡æ»¤æ‰Noneçš„ç‰¹å¾
    comparisons = [(k, t) for k, t in comparisons if features_dict.get(k) is not None]

    n_plots = len(comparisons)
    n_cols = 3
    n_rows = (n_plots + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(7*n_cols, 6*n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    axes = axes.flatten()

    # å¯¹æ¯ç§ç‰¹å¾è¿›è¡Œé™ç»´å’Œå¯è§†åŒ–
    for idx, (feat_key, title) in enumerate(comparisons):
        ax = axes[idx]
        features = features_dict[feat_key]

        print(f"  é™ç»´ {feat_key}...")

        # t-SNEé™ç»´
        if method.lower() == 'tsne':
            embedded = TSNE(n_components=2, random_state=42,
                          perplexity=min(30, len(features)-1)).fit_transform(features)
        else:
            # å¯ä»¥æ‰©å±•æ”¯æŒUMAP
            embedded = TSNE(n_components=2, random_state=42,
                          perplexity=min(30, len(features)-1)).fit_transform(features)

        # ç»˜å›¾
        if is_classification:
            unique_classes = np.unique(targets)
            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_classes)))

            for cls_idx, cls in enumerate(unique_classes):
                mask = targets == cls
                ax.scatter(embedded[mask, 0], embedded[mask, 1],
                          c=[colors[cls_idx]], label=f'Class {int(cls)}',
                          alpha=0.6, s=30, edgecolors='k', linewidth=0.3)
            ax.legend(loc='best', fontsize=8)
        else:
            scatter = ax.scatter(embedded[:, 0], embedded[:, 1],
                               c=targets, cmap='viridis', alpha=0.6,
                               s=30, edgecolors='k', linewidth=0.3)
            plt.colorbar(scatter, ax=ax, label='Target Value')

        ax.set_xlabel('Dimension 1')
        ax.set_ylabel('Dimension 2')
        ax.set_title(title, fontsize=12, weight='bold')
        ax.grid(True, alpha=0.2)

    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(comparisons), len(axes)):
        axes[idx].axis('off')

    plt.suptitle('Feature Space Comparison: Impact of Fusion Mechanisms',
                fontsize=16, weight='bold', y=0.995)
    plt.tight_layout()

    save_path = os.path.join(save_dir, f'feature_comparison_{method}.pdf')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜å¯¹æ¯”å›¾: {save_path}")
    plt.close()


def compute_quantitative_metrics(features_dict, targets, is_classification=False):
    """
    è®¡ç®—å®šé‡è¯„ä¼°æŒ‡æ ‡

    æŒ‡æ ‡:
    - Silhouette Score: èšç±»è´¨é‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼Œ-1åˆ°1ï¼‰
    - Davies-Bouldin Index: èšç±»åˆ†ç¦»åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    - Intra-class similarity: ç±»å†…ç›¸ä¼¼åº¦ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    - Inter-class similarity: ç±»é—´ç›¸ä¼¼åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    """
    print("\nğŸ“Š è®¡ç®—å®šé‡è¯„ä¼°æŒ‡æ ‡...")

    metrics = {}

    for feat_key, features in features_dict.items():
        if features is None:
            continue

        print(f"  è¯„ä¼° {feat_key}...")

        feat_metrics = {}

        # 1. Silhouette Score (éœ€è¦è‡³å°‘2ä¸ªç±»åˆ«)
        if len(np.unique(targets)) > 1:
            try:
                sil_score = silhouette_score(features, targets)
                feat_metrics['silhouette'] = sil_score
            except:
                feat_metrics['silhouette'] = None

        # 2. Davies-Bouldin Index
        if len(np.unique(targets)) > 1:
            try:
                db_score = davies_bouldin_score(features, targets)
                feat_metrics['davies_bouldin'] = db_score
            except:
                feat_metrics['davies_bouldin'] = None

        # 3. ç±»å†…/ç±»é—´ç›¸ä¼¼åº¦ï¼ˆå¯¹äºåˆ†ç±»ä»»åŠ¡ï¼‰
        if is_classification or len(np.unique(targets)) < 50:
            unique_labels = np.unique(targets)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
            sim_matrix = cosine_similarity(features)

            intra_sims = []
            inter_sims = []

            for label in unique_labels:
                mask = targets == label
                indices = np.where(mask)[0]

                if len(indices) > 1:
                    # ç±»å†…ç›¸ä¼¼åº¦
                    intra_sim = sim_matrix[np.ix_(indices, indices)]
                    # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±ï¼‰
                    intra_sim = intra_sim[~np.eye(len(indices), dtype=bool)]
                    if len(intra_sim) > 0:
                        intra_sims.append(np.mean(intra_sim))

                # ç±»é—´ç›¸ä¼¼åº¦
                other_mask = ~mask
                other_indices = np.where(other_mask)[0]
                if len(other_indices) > 0:
                    inter_sim = sim_matrix[np.ix_(indices, other_indices)]
                    inter_sims.append(np.mean(inter_sim))

            feat_metrics['intra_class_sim'] = np.mean(intra_sims) if intra_sims else None
            feat_metrics['inter_class_sim'] = np.mean(inter_sims) if inter_sims else None

            # åˆ†ç¦»åº¦ = ç±»å†…ç›¸ä¼¼åº¦ - ç±»é—´ç›¸ä¼¼åº¦ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            if feat_metrics['intra_class_sim'] and feat_metrics['inter_class_sim']:
                feat_metrics['separation'] = (feat_metrics['intra_class_sim'] -
                                             feat_metrics['inter_class_sim'])

        metrics[feat_key] = feat_metrics

    return metrics


def visualize_metrics_comparison(metrics, save_dir):
    """å¯è§†åŒ–æŒ‡æ ‡å¯¹æ¯”"""
    print("\nğŸ“Š ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾...")

    # å‡†å¤‡æ•°æ®
    metric_names = ['silhouette', 'davies_bouldin', 'intra_class_sim',
                    'inter_class_sim', 'separation']
    metric_labels = ['Silhouetteâ†‘', 'Davies-Bouldinâ†“', 'Intra-class Simâ†‘',
                     'Inter-class Simâ†“', 'Separationâ†‘']

    feat_keys = list(metrics.keys())

    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()

    for idx, (metric_name, metric_label) in enumerate(zip(metric_names, metric_labels)):
        ax = axes[idx]

        values = []
        labels = []
        colors = []

        color_map = {
            'graph_base': '#A8DADC',
            'text_base': '#FFDAB9',
            'graph_middle': '#7FB3D5',
            'graph_fine': '#F4A261',
            'text_fine': '#FFCBA4',
            'graph_cross': '#E76F51',
            'text_cross': '#FFAC86',
            'fused': '#6A4C93'
        }

        for feat_key in feat_keys:
            if metric_name in metrics[feat_key] and metrics[feat_key][metric_name] is not None:
                values.append(metrics[feat_key][metric_name])
                labels.append(feat_key.replace('_', '\n'))
                colors.append(color_map.get(feat_key, '#CCCCCC'))

        if values:
            bars = ax.bar(range(len(values)), values, color=colors,
                         edgecolor='black', linewidth=1.5, alpha=0.8)
            ax.set_xticks(range(len(values)))
            ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)
            ax.set_ylabel(metric_label, fontsize=11, weight='bold')
            ax.set_title(metric_label, fontsize=12, weight='bold')
            ax.grid(axis='y', alpha=0.3, linestyle='--')

            # æ ‡æ³¨æ•°å€¼
            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:.3f}', ha='center', va='bottom', fontsize=9)

    # éšè—æœ€åä¸€ä¸ªç©ºç™½å­å›¾
    axes[-1].axis('off')

    plt.suptitle('Quantitative Metrics Comparison\nâ†‘ Higher is better | â†“ Lower is better',
                fontsize=14, weight='bold')
    plt.tight_layout()

    save_path = os.path.join(save_dir, 'metrics_comparison.pdf')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.savefig(save_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜æŒ‡æ ‡å¯¹æ¯”å›¾: {save_path}")
    plt.close()


def generate_comparison_report(metrics, save_dir):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    print("\nğŸ“ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")

    report_path = os.path.join(save_dir, 'comparison_report.txt')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("èåˆæœºåˆ¶å¯¹æ¯”å®éªŒæŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")

        f.write("## å®éªŒé…ç½®\n\n")
        f.write("å¯¹æ¯”çš„èåˆç­–ç•¥:\n")
        f.write("  1. graph_base: å›¾åŸºç¡€ç‰¹å¾ï¼ˆæ— èåˆï¼‰\n")
        f.write("  2. text_base: æ–‡æœ¬åŸºç¡€ç‰¹å¾ï¼ˆæ— èåˆï¼‰\n")
        f.write("  3. graph_middle: å›¾ç‰¹å¾ + ä¸­é—´èåˆ\n")
        f.write("  4. graph_fine: å›¾ç‰¹å¾ + ç»†ç²’åº¦æ³¨æ„åŠ›\n")
        f.write("  5. graph_cross: å›¾ç‰¹å¾ + å…¨å±€è·¨æ¨¡æ€æ³¨æ„åŠ›\n")
        f.write("  6. fused: å®Œæ•´èåˆï¼ˆæ‰€æœ‰æœºåˆ¶ï¼‰\n\n")

        f.write("## å®šé‡è¯„ä¼°ç»“æœ\n\n")
        f.write("æŒ‡æ ‡è¯´æ˜:\n")
        f.write("  - Silhouette Score: èšç±»è´¨é‡ï¼ˆ-1åˆ°1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰\n")
        f.write("  - Davies-Bouldin Index: èšç±»åˆ†ç¦»åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰\n")
        f.write("  - Intra-class Similarity: ç±»å†…ç›¸ä¼¼åº¦ï¼ˆ0åˆ°1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰\n")
        f.write("  - Inter-class Similarity: ç±»é—´ç›¸ä¼¼åº¦ï¼ˆ0åˆ°1ï¼Œè¶Šä½è¶Šå¥½ï¼‰\n")
        f.write("  - Separation: åˆ†ç¦»åº¦ = ç±»å†…ç›¸ä¼¼åº¦ - ç±»é—´ç›¸ä¼¼åº¦ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰\n\n")

        # åˆ›å»ºè¡¨æ ¼
        f.write("-" * 80 + "\n")
        f.write(f"{'Feature Type':<20} {'Silhouette':>12} {'Davies-B':>12} "
                f"{'Intra-Sim':>12} {'Inter-Sim':>12} {'Separation':>12}\n")
        f.write("-" * 80 + "\n")

        for feat_key, feat_metrics in metrics.items():
            sil = feat_metrics.get('silhouette', None)
            db = feat_metrics.get('davies_bouldin', None)
            intra = feat_metrics.get('intra_class_sim', None)
            inter = feat_metrics.get('inter_class_sim', None)
            sep = feat_metrics.get('separation', None)

            f.write(f"{feat_key:<20} "
                   f"{sil:>12.4f} " if sil else f"{feat_key:<20} {'N/A':>12} ")
            f.write(f"{db:>12.4f} " if db else f"{'N/A':>12} ")
            f.write(f"{intra:>12.4f} " if intra else f"{'N/A':>12} ")
            f.write(f"{inter:>12.4f} " if inter else f"{'N/A':>12} ")
            f.write(f"{sep:>12.4f}\n" if sep else f"{'N/A':>12}\n")

        f.write("-" * 80 + "\n\n")

        # åˆ†ææœ€ä½³é…ç½®
        f.write("## åˆ†æç»“è®º\n\n")

        # æ‰¾å‡ºæœ€ä½³é…ç½®
        if metrics:
            # Silhouetteæœ€é«˜
            sil_scores = {k: v.get('silhouette') for k, v in metrics.items()
                         if v.get('silhouette') is not None}
            if sil_scores:
                best_sil = max(sil_scores.items(), key=lambda x: x[1])
                f.write(f"âœ… Silhouette Scoreæœ€é«˜: {best_sil[0]} ({best_sil[1]:.4f})\n")

            # Davies-Bouldinæœ€ä½
            db_scores = {k: v.get('davies_bouldin') for k, v in metrics.items()
                        if v.get('davies_bouldin') is not None}
            if db_scores:
                best_db = min(db_scores.items(), key=lambda x: x[1])
                f.write(f"âœ… Davies-Bouldinæœ€ä½: {best_db[0]} ({best_db[1]:.4f})\n")

            # Separationæœ€é«˜
            sep_scores = {k: v.get('separation') for k, v in metrics.items()
                         if v.get('separation') is not None}
            if sep_scores:
                best_sep = max(sep_scores.items(), key=lambda x: x[1])
                f.write(f"âœ… Separationæœ€é«˜: {best_sep[0]} ({best_sep[1]:.4f})\n")

        f.write("\n")
        f.write("## å»ºè®®\n\n")
        f.write("1. å¯¹äºéœ€è¦è‰¯å¥½èšç±»è´¨é‡çš„ä»»åŠ¡ï¼Œå»ºè®®ä½¿ç”¨å®Œæ•´èåˆé…ç½®\n")
        f.write("2. å¦‚æœè®¡ç®—èµ„æºæœ‰é™ï¼Œå¯ä»¥åªä½¿ç”¨å…³é”®çš„èåˆæœºåˆ¶\n")
        f.write("3. é€šè¿‡å¯è§†åŒ–å›¾ç‰‡å¯ä»¥æ›´ç›´è§‚åœ°è§‚å¯Ÿç‰¹å¾ç©ºé—´çš„å˜åŒ–\n")

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”ä¸åŒèåˆæœºåˆ¶çš„æ•ˆæœ')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['jarvis', 'mp', 'class'],
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--property', type=str, required=True,
                       help='å±æ€§åç§°')
    parser.add_argument('--root_dir', type=str, default='./dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')

    # å¯é€‰å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./fusion_comparison',
                       help='ä¿å­˜ç»“æœçš„ç›®å½•')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ (cpu æˆ– cuda)')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºåŠ é€Ÿï¼‰')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--split', type=str, default='test',
                       choices=['train', 'val', 'test'],
                       help='ä½¿ç”¨å“ªä¸ªæ•°æ®é›†split')

    args = parser.parse_args()

    os.makedirs(args.save_dir, exist_ok=True)

    print("="*80)
    print("ğŸ”¬ èåˆæœºåˆ¶å¯¹æ¯”å®éªŒ")
    print("="*80)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Dataset: {args.dataset}/{args.property}")
    print(f"Save dir: {args.save_dir}")
    print(f"Max samples: {args.max_samples}")
    print()

    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(args.checkpoint, map_location='cpu', weights_only=False)
    model_config = checkpoint.get('config', None)

    if model_config is None:
        print("âŒ Checkpointä¸­æœªæ‰¾åˆ°æ¨¡å‹é…ç½®")
        return

    model = ALIGNN(model_config)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    is_classification = model_config.classification if hasattr(model_config, 'classification') else False
    print(f"ä»»åŠ¡ç±»å‹: {'åˆ†ç±»' if is_classification else 'å›å½’'}")

    # æ£€æŸ¥æ¨¡å‹æ”¯æŒçš„èåˆæœºåˆ¶
    print(f"\næ¨¡å‹é…ç½®çš„èåˆæœºåˆ¶:")
    print(f"  - å¯¹æ¯”å­¦ä¹ : {model_config.use_contrastive_loss if hasattr(model_config, 'use_contrastive_loss') else False}")
    print(f"  - ä¸­é—´èåˆ: {model_config.use_middle_fusion if hasattr(model_config, 'use_middle_fusion') else False}")
    print(f"  - ç»†ç²’åº¦æ³¨æ„åŠ›: {model_config.use_fine_grained_attention if hasattr(model_config, 'use_fine_grained_attention') else False}")
    print(f"  - å…¨å±€æ³¨æ„åŠ›: {model_config.use_cross_modal_attention if hasattr(model_config, 'use_cross_modal_attention') else False}")

    # 2. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    from train_with_cross_modal_attention import load_dataset, get_dataset_paths

    dataset_mapping = {'jarvis': 'jarvis', 'mp': 'mp', 'class': 'class'}
    actual_dataset = dataset_mapping.get(args.dataset.lower(), args.dataset.lower())

    cif_dir, id_prop_file = get_dataset_paths(args.root_dir, actual_dataset, args.property)
    df = load_dataset(cif_dir, id_prop_file, actual_dataset, args.property)
    print(f"âœ… æ•°æ®é›†å¤§å°: {len(df)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    (train_loader, val_loader, test_loader, _) = get_train_val_loaders(
        dataset='user_data',
        dataset_array=df,
        target='target',
        batch_size=args.batch_size,
        atom_features=model_config.atom_features if hasattr(model_config, 'atom_features') else 'cgcnn',
        neighbor_strategy='k-nearest',
        line_graph=model_config.line_graph if hasattr(model_config, 'line_graph') else True,
        split_seed=42,
        workers=4,
        pin_memory=False,
        save_dataloader=False,
        filename='temp',
        id_tag='jid',
        use_canonize=True,
        cutoff=8.0,
        max_neighbors=12,
        output_dir=args.save_dir
    )

    # é€‰æ‹©æ•°æ®é›†
    if args.split == 'train':
        data_loader = train_loader
    elif args.split == 'val':
        data_loader = val_loader
    else:
        data_loader = test_loader

    print(f"âœ… ä½¿ç”¨{args.split}é›†")

    # 3. æå–ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = 'cpu'

    comparator = FusionComparator(model, device=device)
    features_dict, targets, ids = comparator.extract_features_ablation(
        data_loader, max_samples=args.max_samples
    )

    # 4. å¯è§†åŒ–å¯¹æ¯”
    visualize_feature_comparison(features_dict, targets, args.save_dir,
                                method='tsne', is_classification=is_classification)

    # 5. å®šé‡è¯„ä¼°
    metrics = compute_quantitative_metrics(features_dict, targets,
                                          is_classification=is_classification)

    # 6. å¯è§†åŒ–æŒ‡æ ‡
    visualize_metrics_comparison(metrics, args.save_dir)

    # 7. ç”ŸæˆæŠ¥å‘Š
    generate_comparison_report(metrics, args.save_dir)

    print("\n" + "="*80)
    print("âœ… å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print("="*80)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶åœ¨: {args.save_dir}")
    print("  - feature_comparison_tsne.pdf/png: ç‰¹å¾ç©ºé—´å¯¹æ¯”å¯è§†åŒ–")
    print("  - metrics_comparison.pdf/png: å®šé‡æŒ‡æ ‡å¯¹æ¯”")
    print("  - comparison_report.txt: è¯¦ç»†å¯¹æ¯”æŠ¥å‘Š")


if __name__ == '__main__':
    main()
