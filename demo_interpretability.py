#!/usr/bin/env python
"""
å¯è§£é‡Šæ€§åˆ†ææ¼”ç¤ºè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¼”ç¤ºï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰€æœ‰å¯è§£é‡Šæ€§åŠŸèƒ½ã€‚
å¯ä»¥ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ¥æµ‹è¯•æ‚¨çš„æ¨¡å‹ã€‚

ç”¨æ³•:
    python demo_interpretability.py --checkpoint best_model.pt
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path


def demo_attention_extraction(model, g, lg, text):
    """æ¼”ç¤º1: æå–è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡"""

    print("\n" + "="*80)
    print("æ¼”ç¤º1: æå–è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡")
    print("="*80)

    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        # ğŸ”‘ å…³é”®: ä½¿ç”¨ return_attention=True
        output = model(
            [g.to(device), lg.to(device), text],
            return_features=True,
            return_attention=True
        )

    if isinstance(output, dict):
        print("\nâœ… æˆåŠŸæå–è¾“å‡º!")
        print(f"   åŒ…å«é”®: {list(output.keys())}")

        if 'attention_weights' in output and output['attention_weights'] is not None:
            attn = output['attention_weights']
            print(f"\nâœ… æ³¨æ„åŠ›æƒé‡:")

            if 'graph_to_text' in attn:
                g2t = attn['graph_to_text']
                print(f"   - Graphâ†’Text å½¢çŠ¶: {g2t.shape}")
                print(f"   - Graphâ†’Text å¹³å‡å€¼: {g2t.mean():.4f}")
                print(f"   - Graphâ†’Text èŒƒå›´: [{g2t.min():.4f}, {g2t.max():.4f}]")

            if 'text_to_graph' in attn:
                t2g = attn['text_to_graph']
                print(f"   - Textâ†’Graph å½¢çŠ¶: {t2g.shape}")
                print(f"   - Textâ†’Graph å¹³å‡å€¼: {t2g.mean():.4f}")
                print(f"   - Textâ†’Graph èŒƒå›´: [{t2g.min():.4f}, {t2g.max():.4f}]")

            return output['attention_weights']
        else:
            print("\nâš ï¸  æ³¨æ„åŠ›æƒé‡æœªå¯ç”¨")
            print("   è¯·ç¡®ä¿æ¨¡å‹é…ç½®: use_cross_modal_attention=True")
            return None
    else:
        print("\nâš ï¸  æ¨¡å‹æœªè¿”å›å­—å…¸æ ¼å¼")
        return None


def demo_attention_visualization(attention_weights, save_dir):
    """æ¼”ç¤º2: å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""

    print("\n" + "="*80)
    print("æ¼”ç¤º2: å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡")
    print("="*80)

    if attention_weights is None:
        print("\nâš ï¸  æ²¡æœ‰å¯ç”¨çš„æ³¨æ„åŠ›æƒé‡")
        return

    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # ç®€å•çš„çƒ­å›¾å¯è§†åŒ–
    import seaborn as sns

    fig = plt.figure(figsize=(14, 6))

    # Graph-to-Text
    if 'graph_to_text' in attention_weights:
        ax1 = fig.add_subplot(121)

        g2t = attention_weights['graph_to_text']
        # [batch, heads, 1, 1] -> [heads, 1]
        if g2t.dim() == 4:
            g2t = g2t[0, :, 0, 0].cpu().numpy()
        else:
            g2t = g2t.cpu().numpy()

        sns.heatmap(
            g2t.reshape(-1, 1),
            annot=True,
            fmt='.4f',
            cmap='YlOrRd',
            cbar_kws={'label': 'Attention Weight'},
            ax=ax1,
            yticklabels=[f'Head {i+1}' for i in range(len(g2t))]
        )
        ax1.set_title('Graph â†’ Text Attention\n(å›¾å…³æ³¨æ–‡æœ¬)', fontweight='bold')
        ax1.set_xlabel('Text Features')

    # Text-to-Graph
    if 'text_to_graph' in attention_weights:
        ax2 = fig.add_subplot(122)

        t2g = attention_weights['text_to_graph']
        if t2g.dim() == 4:
            t2g = t2g[0, :, 0, 0].cpu().numpy()
        else:
            t2g = t2g.cpu().numpy()

        sns.heatmap(
            t2g.reshape(-1, 1),
            annot=True,
            fmt='.4f',
            cmap='YlOrRd',
            cbar_kws={'label': 'Attention Weight'},
            ax=ax2,
            yticklabels=[f'Head {i+1}' for i in range(len(t2g))]
        )
        ax2.set_title('Text â†’ Graph Attention\n(æ–‡æœ¬å…³æ³¨å›¾)', fontweight='bold')
        ax2.set_xlabel('Graph Features')

    plt.tight_layout()

    save_path = save_dir / 'attention_weights.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    plt.close()


def demo_atom_importance(model, g, lg, text, atoms, save_dir):
    """æ¼”ç¤º3: è®¡ç®—å’Œå¯è§†åŒ–åŸå­é‡è¦æ€§"""

    print("\n" + "="*80)
    print("æ¼”ç¤º3: åŸå­é‡è¦æ€§åˆ†æ")
    print("="*80)

    device = next(model.parameters()).device
    save_dir = Path(save_dir)

    # æ¢¯åº¦æ³•
    print("\nğŸ”„ ä½¿ç”¨æ¢¯åº¦æ³•è®¡ç®—åŸå­é‡è¦æ€§...")

    model.eval()
    g = g.to(device)
    lg = lg.to(device)

    # å¯ç”¨æ¢¯åº¦
    node_features = g.ndata['atom_features'].clone().detach().requires_grad_(True)
    original_features = g.ndata['atom_features']
    g.ndata['atom_features'] = node_features

    # Forward
    output = model([g, lg, text])

    if isinstance(output, dict):
        prediction = output['predictions']
    else:
        prediction = output

    # Backward
    loss = prediction.sum()
    loss.backward()

    # è®¡ç®—é‡è¦æ€§
    gradients = node_features.grad
    importance = torch.norm(gradients, dim=1).cpu().numpy()

    # æ¢å¤
    g.ndata['atom_features'] = original_features

    print(f"âœ… åŸå­é‡è¦æ€§è®¡ç®—å®Œæˆ")
    print(f"   - åŸå­æ•°: {len(importance)}")
    print(f"   - å¹³å‡é‡è¦æ€§: {importance.mean():.4f}")
    print(f"   - æœ€å¤§é‡è¦æ€§: {importance.max():.4f}")
    print(f"   - æœ€å°é‡è¦æ€§: {importance.min():.4f}")

    # å½’ä¸€åŒ–
    importance_norm = (importance - importance.min()) / (importance.max() - importance.min() + 1e-8)

    # æ‰“å°Top-5
    top_indices = np.argsort(importance_norm)[::-1][:5]
    print(f"\nğŸ“Š Top-5 é‡è¦åŸå­:")
    print(f"{'='*50}")
    for rank, idx in enumerate(top_indices, 1):
        element = atoms.elements[idx]
        score = importance_norm[idx]
        print(f"   {rank}. åŸå­ {idx} ({element}): {score:.4f}")
    print(f"{'='*50}")

    # å¯è§†åŒ–
    print(f"\nğŸ¨ å¯è§†åŒ–åŸå­é‡è¦æ€§...")

    import pandas as pd
    coords = atoms.cart_coords
    elements = list(atoms.elements)

    fig = plt.figure(figsize=(15, 5))

    # 1. åˆ†å¸ƒå›¾
    ax1 = fig.add_subplot(131)
    bars = ax1.bar(range(len(importance_norm)), importance_norm,
                  color=plt.cm.YlOrRd(importance_norm))
    ax1.set_xlabel('Atom Index')
    ax1.set_ylabel('Importance Score')
    ax1.set_title('Atom Importance Distribution')
    ax1.grid(True, alpha=0.3)

    # é«˜äº®top-5
    for idx in top_indices:
        ax1.axvline(idx, color='red', alpha=0.3, linestyle='--')

    # 2. æŒ‰å…ƒç´ ç»Ÿè®¡
    ax2 = fig.add_subplot(132)
    df = pd.DataFrame({
        'Element': elements,
        'Importance': importance_norm
    })
    element_avg = df.groupby('Element')['Importance'].mean().sort_values(ascending=False)

    ax2.barh(range(len(element_avg)), element_avg.values,
            color=plt.cm.viridis(np.linspace(0, 1, len(element_avg))))
    ax2.set_yticks(range(len(element_avg)))
    ax2.set_yticklabels(element_avg.index)
    ax2.set_xlabel('Average Importance')
    ax2.set_title('Importance by Element')
    ax2.grid(True, alpha=0.3, axis='x')

    # 3. ç©ºé—´åˆ†å¸ƒ
    ax3 = fig.add_subplot(133)
    scatter = ax3.scatter(coords[:, 0], coords[:, 1],
                         c=importance_norm, s=300,
                         cmap='YlOrRd', alpha=0.7,
                         edgecolors='black', linewidth=1.5)

    # æ ‡æ³¨top-3
    for idx in top_indices[:3]:
        ax3.annotate(elements[idx],
                    (coords[idx, 0], coords[idx, 1]),
                    fontsize=10, fontweight='bold',
                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

    ax3.set_xlabel('X (Ã…)')
    ax3.set_ylabel('Y (Ã…)')
    ax3.set_title('Spatial Distribution')
    plt.colorbar(scatter, ax=ax3, label='Importance')

    plt.tight_layout()

    save_path = save_dir / 'atom_importance.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… åŸå­é‡è¦æ€§å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    plt.close()

    return importance_norm


def demo_feature_space(model, test_loader, save_dir, num_samples=50):
    """æ¼”ç¤º4: ç‰¹å¾ç©ºé—´å¯è§†åŒ–"""

    print("\n" + "="*80)
    print("æ¼”ç¤º4: ç‰¹å¾ç©ºé—´å¯è§†åŒ–")
    print("="*80)

    device = next(model.parameters()).device
    save_dir = Path(save_dir)

    print(f"\nğŸ”„ æ”¶é›† {num_samples} ä¸ªæ ·æœ¬çš„ç‰¹å¾...")

    all_graph_features = []
    all_text_features = []
    all_labels = []

    model.eval()
    count = 0

    with torch.no_grad():
        for batch in test_loader:
            if count >= num_samples:
                break

            g, lg, text, labels = batch

            output = model(
                [g.to(device), lg.to(device), text],
                return_features=True
            )

            if isinstance(output, dict):
                if 'graph_features' in output and output['graph_features'] is not None:
                    all_graph_features.append(output['graph_features'].cpu())
                if 'text_features' in output and output['text_features'] is not None:
                    all_text_features.append(output['text_features'].cpu())

            all_labels.append(labels.cpu())
            count += len(labels)

            print(f"   æ”¶é›†è¿›åº¦: {count}/{num_samples}", end='\r')

    print(f"\nâœ… ç‰¹å¾æ”¶é›†å®Œæˆ: {count} ä¸ªæ ·æœ¬")

    if not all_graph_features or not all_text_features:
        print("âš ï¸  æ— æ³•æå–ç‰¹å¾ï¼ˆæ¨¡å‹å¯èƒ½æœªè¿”å›ä¸­é—´ç‰¹å¾ï¼‰")
        return

    # åˆå¹¶
    graph_features = torch.cat(all_graph_features, dim=0).numpy()
    text_features = torch.cat(all_text_features, dim=0).numpy()
    labels = torch.cat(all_labels, dim=0).numpy()

    print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
    print(f"   - å›¾ç‰¹å¾å½¢çŠ¶: {graph_features.shape}")
    print(f"   - æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_features.shape}")

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_sim = []
    for i in range(len(graph_features)):
        sim = np.dot(graph_features[i], text_features[i]) / \
              (np.linalg.norm(graph_features[i]) * np.linalg.norm(text_features[i]))
        cosine_sim.append(sim)

    print(f"   - å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {np.mean(cosine_sim):.4f}")
    print(f"   - ç›¸ä¼¼åº¦èŒƒå›´: [{np.min(cosine_sim):.4f}, {np.max(cosine_sim):.4f}]")

    # t-SNEå¯è§†åŒ–
    print(f"\nğŸ¨ ç”Ÿæˆ t-SNE å¯è§†åŒ–...")
    from sklearn.manifold import TSNE

    all_features = np.vstack([graph_features, text_features])
    tsne = TSNE(n_components=2, random_state=42)
    embedded = tsne.fit_transform(all_features)

    n = len(graph_features)
    graph_emb = embedded[:n]
    text_emb = embedded[n:]

    # å¯è§†åŒ–
    fig = plt.figure(figsize=(14, 6))

    # å·¦å›¾: æ¨¡æ€åˆ†ç¦»
    ax1 = fig.add_subplot(121)
    ax1.scatter(graph_emb[:, 0], graph_emb[:, 1],
               c='#3498db', alpha=0.6, s=80, label='Graph',
               marker='o', edgecolors='black', linewidth=0.5)
    ax1.scatter(text_emb[:, 0], text_emb[:, 1],
               c='#e74c3c', alpha=0.6, s=80, label='Text',
               marker='^', edgecolors='black', linewidth=0.5)

    # è¿çº¿
    for i in np.random.choice(n, min(30, n), replace=False):
        ax1.plot([graph_emb[i, 0], text_emb[i, 0]],
                [graph_emb[i, 1], text_emb[i, 1]],
                'gray', alpha=0.2, linewidth=0.8)

    ax1.set_xlabel('t-SNE Component 1')
    ax1.set_ylabel('t-SNE Component 2')
    ax1.set_title('Graph-Text Alignment')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # å³å›¾: æŒ‰æ ‡ç­¾ç€è‰²
    ax2 = fig.add_subplot(122)
    scatter1 = ax2.scatter(graph_emb[:, 0], graph_emb[:, 1],
                          c=labels, cmap='viridis', alpha=0.6, s=80,
                          marker='o', edgecolors='black', linewidth=0.5)
    ax2.scatter(text_emb[:, 0], text_emb[:, 1],
               c=labels, cmap='viridis', alpha=0.6, s=80,
               marker='^', edgecolors='black', linewidth=0.5)

    ax2.set_xlabel('t-SNE Component 1')
    ax2.set_ylabel('t-SNE Component 2')
    ax2.set_title('Features Colored by Target')
    plt.colorbar(scatter1, ax=ax2, label='Target Value')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    save_path = save_dir / 'feature_space_tsne.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ç‰¹å¾ç©ºé—´å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    plt.close()


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""

    print("\n" + "="*80)
    print("ğŸ” å¯è§£é‡Šæ€§åˆ†ææ¼”ç¤º")
    print("="*80)

    # è¿™é‡Œéœ€è¦åŠ è½½æ‚¨çš„å®é™…æ¨¡å‹å’Œæ•°æ®
    print("\nâš ï¸  æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºè„šæœ¬æ¨¡æ¿")
    print("è¯·ä¿®æ”¹ä»¥ä¸‹éƒ¨åˆ†ä»¥é€‚é…æ‚¨çš„å®é™…æ¨¡å‹å’Œæ•°æ®:\n")

    print("TODO:")
    print("1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("2. åŠ è½½æµ‹è¯•æ•°æ®")
    print("3. å‡†å¤‡ä¸€ä¸ªæ ·æœ¬ç”¨äºæ¼”ç¤º")
    print("4. è¿è¡Œæ¼”ç¤ºå‡½æ•°\n")

    # ç¤ºä¾‹ä»£ç ï¼ˆéœ€è¦æ›¿æ¢ï¼‰:
    """
    # 1. åŠ è½½æ¨¡å‹
    checkpoint = torch.load('best_model.pt')
    model = ALIGNN(config.model)
    model.load_state_dict(checkpoint['model'])
    model.eval()

    # 2. åŠ è½½æ•°æ®
    test_loader = ...

    # 3. è·å–ä¸€ä¸ªæ ·æœ¬
    g, lg, text, label = next(iter(test_loader))
    atoms = ...  # ä»æ•°æ®ä¸­è·å–Atomså¯¹è±¡

    # 4. è¿è¡Œæ¼”ç¤º
    save_dir = Path('./demo_results')

    # æ¼”ç¤º1: æ³¨æ„åŠ›æå–
    attention = demo_attention_extraction(model, g, lg, text)

    # æ¼”ç¤º2: æ³¨æ„åŠ›å¯è§†åŒ–
    demo_attention_visualization(attention, save_dir)

    # æ¼”ç¤º3: åŸå­é‡è¦æ€§
    demo_atom_importance(model, g, lg, text, atoms, save_dir)

    # æ¼”ç¤º4: ç‰¹å¾ç©ºé—´
    demo_feature_space(model, test_loader, save_dir, num_samples=50)

    print(f"\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
    """

    print("="*80)


if __name__ == '__main__':
    main()
