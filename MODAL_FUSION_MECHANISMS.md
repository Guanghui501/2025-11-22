# å¤šæ¨¡æ€ç‰¹å¾èåˆæœºåˆ¶è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜åœ¨ä¸åŒæ¨¡æ€æ­é…ä½œç”¨ä¸‹ï¼Œå›¾ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾çš„å˜åŒ–è¿‡ç¨‹ã€‚

---

## ğŸ“Š æ•´ä½“æ¶æ„æµç¨‹

```
æ–‡æœ¬è¾“å…¥ â†’ MatSciBERTç¼–ç  â†’ CLSæŠ•å½± â†’ è·¨æ¨¡æ€äº¤äº’ â†’ èåˆé¢„æµ‹
   â†“                                        â†‘
å›¾ç»“æ„ â†’ åŸå­åµŒå…¥ â†’ ALIGNNå±‚ â†’ GCNå±‚ â†’ å›¾æŠ•å½±
```

---

## 1ï¸âƒ£ åŸºç¡€ç‰¹å¾æå–é˜¶æ®µï¼ˆæ— äº¤äº’ï¼‰

### å›¾ç‰¹å¾æµç¨‹
**ä½ç½®**: `alignn.py:883-903`

```python
# 1. åŸå­ç‰¹å¾åµŒå…¥ [total_atoms, 92] â†’ [total_atoms, 256]
x = self.atom_embedding(x)

# 2. ALIGNNå±‚æ›´æ–°ï¼ˆ4å±‚ï¼‰ï¼šèŠ‚ç‚¹-è¾¹-è§’åº¦ç‰¹å¾è”åˆæ›´æ–°
for alignn_layer in self.alignn_layers:
    x, y, z = alignn_layer(g, lg, x, y, z)

# 3. GCNå±‚æ›´æ–°ï¼ˆ4å±‚ï¼‰ï¼šèŠ‚ç‚¹-è¾¹ç‰¹å¾æ›´æ–°
for gcn_layer in self.gcn_layers:
    x, y = gcn_layer(g, x, y)

# 4. å›¾çº§è¡¨ç¤º [total_atoms, 256] â†’ [batch, 256] â†’ [batch, 64]
graph_emb = self.readout(g, x)  # å¹³å‡æ± åŒ–
h = self.graph_projection(graph_emb)  # æŠ•å½±åˆ°64ç»´
```

**ç‰¹å¾ç»´åº¦å˜åŒ–**:
- è¾“å…¥: `[total_atoms, 92]` (åŸå­ç‰¹å¾)
- åµŒå…¥å: `[total_atoms, 256]`
- æ± åŒ–å: `[batch_size, 256]`
- **æœ€ç»ˆå›¾ç‰¹å¾**: `[batch_size, 64]` âœ…

### æ–‡æœ¬ç‰¹å¾æµç¨‹
**ä½ç½®**: `alignn.py:866-880`

```python
# 1. æ–‡æœ¬æ ‡å‡†åŒ–
norm_sents = [normalize(s) for s in text]

# 2. MatSciBERTç¼–ç  [batch, seq_len] â†’ [batch, seq_len, 768]
encodings = tokenizer(norm_sents, return_tensors='pt', padding=True)
last_hidden_state = text_model(**encodings)[0]

# 3. æå–CLS token + æŠ•å½± [batch, 768] â†’ [batch, 64]
cls_emb = last_hidden_state[:, 0, :]
text_emb = self.text_projection(cls_emb)
```

**ç‰¹å¾ç»´åº¦å˜åŒ–**:
- BERTè¾“å‡º: `[batch_size, seq_len, 768]`
- CLS token: `[batch_size, 768]`
- **æœ€ç»ˆæ–‡æœ¬ç‰¹å¾**: `[batch_size, 64]` âœ…

**æ­¤é˜¶æ®µç‰¹ç‚¹**:
- âœ… å›¾å’Œæ–‡æœ¬**å®Œå…¨ç‹¬ç«‹**ç¼–ç ï¼Œæ— äº¤äº’
- âœ… ä¸¤è€…éƒ½è¢«æŠ•å½±åˆ°ç›¸åŒç»´åº¦ç©ºé—´ï¼ˆ64ç»´ï¼‰
- âœ… è¿™æ˜¯ `visualize_latent_space.py` ä¸­ `'graph'` å’Œ `'text'` ç‰¹å¾çš„æ¥æº

---

## 2ï¸âƒ£ å¯¹æ¯”å­¦ä¹ æœºåˆ¶ (Contrastive Loss)

**å¯ç”¨æ¡ä»¶**: `config.use_contrastive_loss = True`
**ä½ç½®**: `alignn.py:69-118`, `alignn.py:1008-1012`

### ä½œç”¨æœºåˆ¶

```python
class ContrastiveLoss(nn.Module):
    def forward(self, graph_features, text_features):
        # 1. L2å½’ä¸€åŒ–ï¼šå°†ç‰¹å¾æŠ•å½±åˆ°å•ä½è¶…çƒé¢
        graph_features = F.normalize(graph_features, dim=1)  # [batch, 64]
        text_features = F.normalize(text_features, dim=1)    # [batch, 64]

        # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [batch, batch]
        similarity_matrix = (graph_features @ text_features.T) / temperature

        # 3. å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆåŒå‘ï¼‰
        labels = torch.arange(batch_size)  # å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
        loss_g2t = CrossEntropy(similarity_matrix, labels)    # å›¾æ‰¾æ–‡æœ¬
        loss_t2g = CrossEntropy(similarity_matrix.T, labels)  # æ–‡æœ¬æ‰¾å›¾

        return (loss_g2t + loss_t2g) / 2
```

### ç‰¹å¾å˜åŒ–

**å¯¹å›¾ç‰¹å¾çš„å½±å“**:
```
åŸå§‹å›¾ç‰¹å¾ [batch, 64]
    â†“ L2å½’ä¸€åŒ–
å½’ä¸€åŒ–å›¾ç‰¹å¾ [batch, 64] (å•ä½å‘é‡)
    â†“ å¯¹æ¯”æŸå¤±æ¢¯åº¦
å›¾ç‰¹å¾è¢«æ‹‰å‘å¯¹åº”çš„æ–‡æœ¬ç‰¹å¾ï¼Œè¿œç¦»ä¸å¯¹åº”çš„æ–‡æœ¬
```

**å¯¹æ–‡æœ¬ç‰¹å¾çš„å½±å“**:
```
åŸå§‹æ–‡æœ¬ç‰¹å¾ [batch, 64]
    â†“ L2å½’ä¸€åŒ–
å½’ä¸€åŒ–æ–‡æœ¬ç‰¹å¾ [batch, 64] (å•ä½å‘é‡)
    â†“ å¯¹æ¯”æŸå¤±æ¢¯åº¦
æ–‡æœ¬ç‰¹å¾è¢«æ‹‰å‘å¯¹åº”çš„å›¾ç‰¹å¾ï¼Œè¿œç¦»ä¸å¯¹åº”çš„å›¾
```

### æ•ˆæœç¤ºæ„

å‡è®¾æ‰¹æ¬¡å¤§å°ä¸º3ï¼ŒåŒ…å« (å›¾1,æ–‡æœ¬1), (å›¾2,æ–‡æœ¬2), (å›¾3,æ–‡æœ¬3)

**ç›¸ä¼¼åº¦çŸ©é˜µ** (è®­ç»ƒåæœŸæœ›çŠ¶æ€):
```
           æ–‡æœ¬1  æ–‡æœ¬2  æ–‡æœ¬3
å›¾1    â†’   [é«˜]   ä½    ä½     â† æ­£æ ·æœ¬å¯¹é½
å›¾2    â†’    ä½   [é«˜]   ä½     â† æ­£æ ·æœ¬å¯¹é½
å›¾3    â†’    ä½    ä½   [é«˜]    â† æ­£æ ·æœ¬å¯¹é½
```

**æ ¸å¿ƒæ•ˆæœ**:
- ğŸ¯ **æ‹‰è¿‘**: å¯¹åº”çš„å›¾-æ–‡æœ¬å¯¹åœ¨ç‰¹å¾ç©ºé—´ä¸­è·ç¦»æ›´è¿‘
- ğŸš« **æ¨è¿œ**: ä¸å¯¹åº”çš„å›¾-æ–‡æœ¬å¯¹è·ç¦»æ›´è¿œ
- ğŸ“ **å¯¹é½**: ä¸¤ä¸ªæ¨¡æ€åœ¨åŒä¸€ç‰¹å¾ç©ºé—´ä¸­è¯­ä¹‰å¯¹é½
- âš ï¸ **ä¸æ”¹å˜ç‰¹å¾ç»´åº¦**: ä»ç„¶æ˜¯ `[batch, 64]`ï¼Œä½†ç‰¹å¾åˆ†å¸ƒæ›´æœ‰ç»“æ„

**å¯è§†åŒ–å½±å“**:
- åœ¨ t-SNE/UMAP å¯è§†åŒ–ä¸­ï¼Œå¯¹åº”çš„å›¾-æ–‡æœ¬å¯¹ä¼šèšé›†åœ¨ä¸€èµ·
- ä¸åŒææ–™çš„å›¾-æ–‡æœ¬ç°‡ä¼šæ›´åŠ åˆ†ç¦»

---

## 3ï¸âƒ£ ä¸­é—´èåˆæœºåˆ¶ (Middle Fusion)

**å¯ç”¨æ¡ä»¶**: `config.use_middle_fusion = True`
**ä½ç½®**: `alignn.py:121-218`, `alignn.py:896-899`

### ä½œç”¨æ—¶æœº

åœ¨ **ALIGNN å±‚å†…éƒ¨**è¿›è¡Œèåˆï¼ˆå›¾ç¼–ç è¿‡ç¨‹ä¸­ï¼‰ï¼š

```python
# ALIGNNå±‚å¾ªç¯
for idx, alignn_layer in enumerate(self.alignn_layers):
    x, y, z = alignn_layer(g, lg, x, y, z)  # å›¾æ›´æ–°

    # å¦‚æœå½“å‰å±‚é…ç½®äº†ä¸­é—´èåˆï¼ˆå¦‚ç¬¬2å±‚ï¼‰
    if idx in self.middle_fusion_layer_indices:  # ä¾‹å¦‚ idx=2
        x = self.middle_fusion_modules[f'layer_{idx}'](x, text_emb, batch_num_nodes)
```

### èåˆæœºåˆ¶è¯¦è§£

**MiddleFusionModule å†…éƒ¨æµç¨‹**:

```python
# è¾“å…¥:
#   node_feat: [total_atoms, 256] - å½“å‰å±‚çš„èŠ‚ç‚¹ç‰¹å¾
#   text_feat: [batch, 64] - æ–‡æœ¬ç‰¹å¾ï¼ˆå·²æŠ•å½±ï¼‰

# 1. æ–‡æœ¬ç‰¹å¾å˜æ¢ [batch, 64] â†’ [batch, 256]
text_transformed = self.text_transform(text_feat)

# 2. å¹¿æ’­æ–‡æœ¬ç‰¹å¾åˆ°æ¯ä¸ªåŸå­
#    æ ¹æ®æ¯ä¸ªå›¾çš„åŸå­æ•°ï¼Œå°†æ–‡æœ¬ç‰¹å¾å¤åˆ¶ç»™å¯¹åº”çš„åŸå­
#    [batch, 256] â†’ [total_atoms, 256]
text_broadcasted = æŒ‰å›¾é‡å¤(text_transformed, batch_num_nodes)

# 3. é—¨æ§èåˆæœºåˆ¶
gate_input = concat([node_feat, text_broadcasted], dim=-1)  # [total_atoms, 512]
gate_values = sigmoid(Linear(gate_input))  # [total_atoms, 256]

# 4. é—¨æ§æ›´æ–° + æ®‹å·®è¿æ¥
enhanced = node_feat + gate_values * text_broadcasted
enhanced = LayerNorm(Dropout(enhanced))

# è¾“å‡º: [total_atoms, 256] - æ–‡æœ¬å¢å¼ºçš„èŠ‚ç‚¹ç‰¹å¾
```

### ç‰¹å¾å˜åŒ–

**å¯¹å›¾ç‰¹å¾ï¼ˆèŠ‚ç‚¹ç‰¹å¾ï¼‰çš„å½±å“**:
```
ç¬¬2å±‚ALIGNNè¾“å‡º: [total_atoms, 256]
    â†“
æ–‡æœ¬ä¿¡æ¯æ³¨å…¥ï¼ˆé—¨æ§åŠ æƒï¼‰
    â†“
æ–‡æœ¬å¢å¼ºçš„èŠ‚ç‚¹ç‰¹å¾: [total_atoms, 256]
    â†“ ç»§ç»­åç»­ALIGNN/GCNå±‚
æœ€ç»ˆå›¾ç‰¹å¾: [batch, 64]
```

**å¯¹æ–‡æœ¬ç‰¹å¾çš„å½±å“**:
- âŒ **ä¸ç›´æ¥æ”¹å˜æ–‡æœ¬ç‰¹å¾**
- âœ… æ–‡æœ¬é€šè¿‡é—¨æ§æœºåˆ¶è°ƒåˆ¶å›¾ç¼–ç è¿‡ç¨‹
- âœ… æ–‡æœ¬ä¿¡æ¯éšå¼åœ°èå…¥åˆ°å›¾çš„èŠ‚ç‚¹è¡¨ç¤ºä¸­

### é—¨æ§æœºåˆ¶æ•ˆæœ

**é—¨æ§å€¼çš„å«ä¹‰**:
```python
gate_values = sigmoid(Linear([node_feat, text_feat]))  # èŒƒå›´ [0, 1]
```

- `gate_values â‰ˆ 1`: è¯¥åŸå­èŠ‚ç‚¹**å¼ºçƒˆæ¥å—**æ–‡æœ¬ä¿¡æ¯
- `gate_values â‰ˆ 0`: è¯¥åŸå­èŠ‚ç‚¹**å¿½ç•¥**æ–‡æœ¬ä¿¡æ¯
- æ¯ä¸ªåŸå­æ ¹æ®è‡ªèº«ç‰¹å¾å’Œæ–‡æœ¬å†…å®¹**è‡ªé€‚åº”å†³å®š**æ¥å—å¤šå°‘æ–‡æœ¬å½±å“

**ç¤ºä¾‹**:
```
æ–‡æœ¬: "This is a high-temperature superconductor with copper-oxygen layers"

åŸå­ç‰¹å¾ + é—¨æ§å€¼:
CuåŸå­: gate=0.9 â†’ å¼ºçƒˆæ¥å—æ–‡æœ¬ä¸­"copper"çš„è¯­ä¹‰ä¿¡æ¯
OåŸå­:  gate=0.8 â†’ æ¥å—"oxygen"ç›¸å…³ä¿¡æ¯
SiåŸå­: gate=0.2 â†’ å¼±æ¥å—ï¼ˆæ–‡æœ¬æœªæåŠï¼‰
```

### æ ¸å¿ƒæ•ˆæœ

- ğŸ”„ **å•å‘èåˆ**: æ–‡æœ¬ â†’ å›¾ï¼ˆæ–‡æœ¬è°ƒåˆ¶å›¾ç¼–ç ï¼‰
- â° **æ—©æœŸèåˆ**: åœ¨å›¾ç¼–ç **ä¸­é—´å±‚**ä»‹å…¥ï¼Œä¸æ˜¯æœ€åæ‰èåˆ
- ğŸ¯ **åŸå­çº§ç²¾ç»†æ§åˆ¶**: æ¯ä¸ªåŸå­ç‹¬ç«‹å†³å®šæ¥å—å¤šå°‘æ–‡æœ¬ä¿¡æ¯
- ğŸ“ **å±€éƒ¨æ€§**: æ–‡æœ¬ä¿¡æ¯é€šè¿‡åç»­å›¾å·ç§¯å±‚ä¼ æ’­åˆ°æ•´ä¸ªå›¾

---

## 4ï¸âƒ£ ç»†ç²’åº¦è·¨æ¨¡æ€æ³¨æ„åŠ› (Fine-grained Cross-modal Attention)

**å¯ç”¨æ¡ä»¶**: `config.use_fine_grained_attention = True`
**ä½ç½®**: `alignn.py:352-528`, `alignn.py:905-952`

### ä½œç”¨æ—¶æœº

åœ¨ **æ‰€æœ‰å›¾å±‚ä¹‹åï¼Œæ± åŒ–ä¹‹å‰**ï¼š

```python
# å®Œæˆæ‰€æœ‰ALIGNNå’ŒGCNå±‚
for gcn_layer in self.gcn_layers:
    x, y = gcn_layer(g, x, y)
# æ­¤æ—¶ x: [total_atoms, 256]

# ç»†ç²’åº¦æ³¨æ„åŠ›
enhanced_nodes, enhanced_tokens = self.fine_grained_attention(
    node_features_batched,  # [batch, max_atoms, 256]
    text_tokens,            # [batch, seq_len, 768] (BERTæ‰€æœ‰token)
    node_mask=node_mask,
    token_mask=attention_mask
)

# ç»§ç»­æ± åŒ–
graph_emb = self.readout(g, x_enhanced)
```

### åŒå‘æ³¨æ„åŠ›æœºåˆ¶

#### **åŸå­â†’æ–‡æœ¬æ³¨æ„åŠ›** (Atom-to-Token)

```python
# 1. è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼
Q_a2t = Linear(node_feat)    # [batch, max_atoms, hidden] - åŸå­æŸ¥è¯¢
K_a2t = Linear(token_feat)   # [batch, seq_len, hidden]   - æ–‡æœ¬é”®
V_a2t = Linear(token_feat)   # [batch, seq_len, hidden]   - æ–‡æœ¬å€¼

# 2. å¤šå¤´æ³¨æ„åŠ›å¾—åˆ† [batch, heads, max_atoms, seq_len]
attn_a2t = softmax((Q_a2t @ K_a2t.T) / sqrt(head_dim))

# 3. åŠ æƒèšåˆæ–‡æœ¬ä¿¡æ¯
context_a2t = attn_a2t @ V_a2t  # [batch, max_atoms, hidden]

# 4. æ®‹å·®è¿æ¥
enhanced_nodes = LayerNorm(node_feat + context_a2t)
```

**å«ä¹‰**: æ¯ä¸ªåŸå­å…³æ³¨æ–‡æœ¬ä¸­çš„å“ªäº›è¯ï¼Ÿ

**ç¤ºä¾‹**:
```
ææ–™æè¿°: "Silicon dioxide with high thermal stability"
åŸå­åºåˆ—: [Siâ‚, Oâ‚, Oâ‚‚, Siâ‚‚, ...]

æ³¨æ„åŠ›æƒé‡çŸ©é˜µ [atoms Ã— tokens]:
          Silicon  dioxide  with  high  thermal  stability
Siâ‚        0.7     0.2     0.0   0.0    0.1      0.0      â† SiåŸå­å…³æ³¨"Silicon"
Oâ‚         0.1     0.6     0.0   0.0    0.2      0.1      â† OåŸå­å…³æ³¨"dioxide", "thermal"
Oâ‚‚         0.1     0.6     0.0   0.0    0.2      0.1
Siâ‚‚        0.7     0.2     0.0   0.0    0.1      0.0
```

#### **æ–‡æœ¬â†’åŸå­æ³¨æ„åŠ›** (Token-to-Atom)

```python
# 1. è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼
Q_t2a = Linear(token_feat)   # [batch, seq_len, hidden]   - æ–‡æœ¬æŸ¥è¯¢
K_t2a = Linear(node_feat)    # [batch, max_atoms, hidden] - åŸå­é”®
V_t2a = Linear(node_feat)    # [batch, max_atoms, hidden] - åŸå­å€¼

# 2. å¤šå¤´æ³¨æ„åŠ›å¾—åˆ† [batch, heads, seq_len, max_atoms]
attn_t2a = softmax((Q_t2a @ K_t2a.T) / sqrt(head_dim))

# 3. åŠ æƒèšåˆåŸå­ä¿¡æ¯
context_t2a = attn_t2a @ V_t2a  # [batch, seq_len, hidden]

# 4. æ®‹å·®è¿æ¥
enhanced_tokens = LayerNorm(token_feat + context_t2a)
```

**å«ä¹‰**: æ¯ä¸ªæ–‡æœ¬è¯å…³æ³¨ç»“æ„ä¸­çš„å“ªäº›åŸå­ï¼Ÿ

**ç¤ºä¾‹**:
```
æ³¨æ„åŠ›æƒé‡çŸ©é˜µ [tokens Ã— atoms]:
           Siâ‚   Oâ‚   Oâ‚‚   Siâ‚‚  ...
Silicon    0.8   0.0  0.0  0.7  ...  â† "Silicon"è¯å…³æ³¨SiåŸå­
dioxide    0.1   0.4  0.4  0.1  ...  â† "dioxide"è¯å…³æ³¨OåŸå­
thermal    0.2   0.2  0.2  0.2  ...  â† "thermal"è¯å‡åŒ€å…³æ³¨
stability  0.15  0.2  0.2  0.15 ...
```

### ç‰¹å¾å˜åŒ–

**å¯¹å›¾ç‰¹å¾ï¼ˆåŸå­çº§ï¼‰çš„å½±å“**:
```
åŸå§‹åŸå­ç‰¹å¾: [batch, max_atoms, 256]
    â†“
åŸå­â†’æ–‡æœ¬æ³¨æ„åŠ›
    â†“
æ–‡æœ¬å¢å¼ºçš„åŸå­ç‰¹å¾: [batch, max_atoms, 256]
    â†“ è½¬å›DGLæ ¼å¼ + æ± åŒ–
å¢å¼ºçš„å›¾çº§ç‰¹å¾: [batch, 64]
```

**å¯¹æ–‡æœ¬ç‰¹å¾ï¼ˆtokençº§ï¼‰çš„å½±å“**:
```
åŸå§‹tokenç‰¹å¾: [batch, seq_len, 768]
    â†“
æ–‡æœ¬â†’åŸå­æ³¨æ„åŠ›
    â†“
ç»“æ„å¢å¼ºçš„tokenç‰¹å¾: [batch, seq_len, 768]
    â†“ (å¦‚æœéœ€è¦)å¯ä»¥é‡æ–°æ± åŒ–
å¢å¼ºçš„æ–‡æœ¬ç‰¹å¾
```

### æ ¸å¿ƒæ•ˆæœ

- ğŸ”„ **åŒå‘èåˆ**: å›¾ â†” æ–‡æœ¬ï¼ˆäº’ç›¸å¢å¼ºï¼‰
- ğŸ”¬ **ç»†ç²’åº¦**: åŸå­-è¯çº§åˆ«çš„ç²¾ç¡®å¯¹é½
- ğŸ¯ **å¯è§£é‡Šæ€§**: å¯ä»¥å¯è§†åŒ–å“ªä¸ªåŸå­å…³æ³¨å“ªä¸ªè¯
- â° **æ™šæœŸèåˆ**: åœ¨å›¾ç¼–ç å®Œæˆåè¿›è¡Œ
- ğŸ“Š **ç‰¹å¾ç»´åº¦ä¸å˜**: åŸå­ç‰¹å¾ä»ä¸º256ç»´ï¼Œtokenç‰¹å¾ä»ä¸º768ç»´

---

## 5ï¸âƒ£ å…¨å±€è·¨æ¨¡æ€æ³¨æ„åŠ› (Cross-modal Attention)

**å¯ç”¨æ¡ä»¶**: `config.use_cross_modal_attention = True`
**ä½ç½®**: `alignn.py:221-349`, `alignn.py:960-971`

### ä½œç”¨æ—¶æœº

åœ¨ **å›¾æ± åŒ–å’ŒæŠ•å½±ä¹‹å**ï¼ˆæœ€ç»ˆèåˆé˜¶æ®µï¼‰ï¼š

```python
# 1. å›¾ç‰¹å¾å·²ç»æ± åŒ–å¹¶æŠ•å½±
graph_emb = self.readout(g, x)      # [batch, 256]
h = self.graph_projection(graph_emb)  # [batch, 64]

# 2. æ–‡æœ¬ç‰¹å¾å·²ç»æŠ•å½±
text_emb = self.text_projection(cls_emb)  # [batch, 64]

# 3. å…¨å±€è·¨æ¨¡æ€æ³¨æ„åŠ›
enhanced_graph, enhanced_text = self.cross_modal_attention(h, text_emb)

# 4. å¹³å‡èåˆ
h = (enhanced_graph + enhanced_text) / 2  # [batch, 64]
out = FC(h)  # æœ€ç»ˆé¢„æµ‹
```

### åŒå‘æ³¨æ„åŠ›æœºåˆ¶

#### **å›¾â†’æ–‡æœ¬æ³¨æ„åŠ›** (Graph-to-Text)

```python
# è¾“å…¥éƒ½æ˜¯å…¨å±€ç‰¹å¾ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªå‘é‡ï¼‰
# graph_feat: [batch, 64]
# text_feat: [batch, 64]

# 1. æ·»åŠ åºåˆ—ç»´åº¦
graph_feat_seq = graph_feat.unsqueeze(1)  # [batch, 1, 64]
text_feat_seq = text_feat.unsqueeze(1)    # [batch, 1, 64]

# 2. è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼ï¼ˆå¤šå¤´ï¼‰
Q_g2t = self.g2t_query(graph_feat_seq)  # [batch, 1, 256]
K_g2t = self.g2t_key(text_feat_seq)     # [batch, 1, 256]
V_g2t = self.g2t_value(text_feat_seq)   # [batch, 1, 256]

# 3. å¤šå¤´æ³¨æ„åŠ›
attn_g2t = softmax((Q_g2t @ K_g2t.T) / sqrt(head_dim))  # [batch, heads, 1, 1]
context_g2t = attn_g2t @ V_g2t  # [batch, 1, 256]
context_g2t = Linear(context_g2t)  # [batch, 64]

# 4. æ®‹å·® + å½’ä¸€åŒ–
enhanced_graph = LayerNorm(graph_feat + context_g2t)  # [batch, 64]
```

#### **æ–‡æœ¬â†’å›¾æ³¨æ„åŠ›** (Text-to-Graph)

```python
# å¯¹ç§°æ“ä½œ
Q_t2g = self.t2g_query(text_feat_seq)   # [batch, 1, 256]
K_t2g = self.t2g_key(graph_feat_seq)    # [batch, 1, 256]
V_t2g = self.t2g_value(graph_feat_seq)  # [batch, 1, 256]

attn_t2g = softmax((Q_t2g @ K_t2g.T) / sqrt(head_dim))
context_t2g = attn_t2g @ V_t2g
context_t2g = Linear(context_t2g)  # [batch, 64]

enhanced_text = LayerNorm(text_feat + context_t2g)  # [batch, 64]
```

### ç‰¹å¾å˜åŒ–

**å¯¹å›¾ç‰¹å¾çš„å½±å“**:
```
åŸå§‹å›¾ç‰¹å¾: [batch, 64]
    â†“
å›¾â†’æ–‡æœ¬æ³¨æ„åŠ›ï¼ˆå›¾æŸ¥è¯¢æ–‡æœ¬ä¿¡æ¯ï¼‰
    â†“
æ–‡æœ¬å¢å¼ºçš„å›¾ç‰¹å¾: [batch, 64]
    â†“
ä¸å¢å¼ºæ–‡æœ¬ç‰¹å¾å¹³å‡
    â†“
èåˆç‰¹å¾: [batch, 64] â†’ ç”¨äºæœ€ç»ˆé¢„æµ‹
```

**å¯¹æ–‡æœ¬ç‰¹å¾çš„å½±å“**:
```
åŸå§‹æ–‡æœ¬ç‰¹å¾: [batch, 64]
    â†“
æ–‡æœ¬â†’å›¾æ³¨æ„åŠ›ï¼ˆæ–‡æœ¬æŸ¥è¯¢å›¾ä¿¡æ¯ï¼‰
    â†“
å›¾å¢å¼ºçš„æ–‡æœ¬ç‰¹å¾: [batch, 64]
    â†“
ä¸å¢å¼ºå›¾ç‰¹å¾å¹³å‡
    â†“
èåˆç‰¹å¾: [batch, 64] â†’ ç”¨äºæœ€ç»ˆé¢„æµ‹
```

### æ³¨æ„åŠ›æƒé‡å«ä¹‰

ç”±äºåºåˆ—é•¿åº¦ä¸º1ï¼ˆå…¨å±€ç‰¹å¾ï¼‰ï¼Œæ³¨æ„åŠ›æƒé‡æœ¬è´¨ä¸Šæ˜¯**æ ‡é‡ç›¸ä¼¼åº¦**:

```python
attn_g2t: [batch, heads, 1, 1]  # å›¾å¯¹æ–‡æœ¬çš„å…³æ³¨åº¦ï¼ˆæ¥è¿‘1.0ï¼‰
attn_t2g: [batch, heads, 1, 1]  # æ–‡æœ¬å¯¹å›¾çš„å…³æ³¨åº¦ï¼ˆæ¥è¿‘1.0ï¼‰
```

**æ•ˆæœ**:
- ä¸æ˜¯é€‰æ‹©æ€§æ³¨æ„ï¼ˆåƒç»†ç²’åº¦é‚£æ ·ï¼‰ï¼Œè€Œæ˜¯**å…¨å±€ä¿¡æ¯èåˆ**
- ç›¸å½“äºå­¦ä¹ ä¸€ä¸ªè‡ªé€‚åº”çš„èåˆæƒé‡

### æ ¸å¿ƒæ•ˆæœ

- ğŸ”„ **åŒå‘èåˆ**: å›¾ â†” æ–‡æœ¬ï¼ˆäº’ç›¸å¢å¼ºï¼‰
- ğŸŒ **å…¨å±€çº§åˆ«**: æ•´ä¸ªå›¾å’Œæ•´ä¸ªæ–‡æœ¬çš„äº¤äº’
- â° **æœ€æ™šèåˆ**: åœ¨æ‰€æœ‰ç¼–ç å®Œæˆå
- ğŸ¯ **å¯¹ç§°æ€§**: å›¾å’Œæ–‡æœ¬å¹³ç­‰åœ°ç›¸äº’å¢å¼º
- ğŸ“Š **ç»´åº¦ä¿æŒ**: èåˆåä»ä¸º `[batch, 64]`

---

## ğŸ“ˆ å¯è§†åŒ–ç‰¹å¾å¯¹åº”å…³ç³»

åœ¨ `visualize_latent_space.py` ä¸­æå–çš„ç‰¹å¾ï¼š

| ç‰¹å¾ç±»å‹ | æ¥æºä½ç½® | åŒ…å«çš„äº¤äº’ | ç»´åº¦ |
|---------|---------|-----------|-----|
| `'graph'` | `graph_features` | å¯èƒ½åŒ…å«ä¸­é—´èåˆã€ç»†ç²’åº¦æ³¨æ„åŠ›çš„å½±å“ | 64 |
| `'text'` | `text_features` | å¯èƒ½åŒ…å«ç»†ç²’åº¦æ³¨æ„åŠ›çš„å½±å“ | 64 |
| `'fused'` | `concat([graph, text])` | ç®€å•æ‹¼æ¥ä¸Šè¿°ä¸¤è€… | 128 |

**æ³¨æ„**:
- å¦‚æœ `use_cross_modal_attention=True`ï¼Œè¿”å›çš„æ˜¯ `enhanced_graph` å’Œ `enhanced_text`
- å¦‚æœå¯ç”¨äº†ç»†ç²’åº¦æ³¨æ„åŠ›ï¼Œå›¾ç‰¹å¾å·²ç»èåˆäº†æ–‡æœ¬ä¿¡æ¯

---

## ğŸ¯ å„æœºåˆ¶å¯¹æ¯”æ€»ç»“

| æœºåˆ¶ | èåˆæ—¶æœº | èåˆç²’åº¦ | æ–¹å‘ | ç‰¹å¾ç»´åº¦å˜åŒ– | ä¸»è¦ä½œç”¨ |
|-----|---------|---------|-----|-------------|---------|
| **å¯¹æ¯”å­¦ä¹ ** | è®­ç»ƒæ—¶ï¼ˆæŸå¤±å‡½æ•°ï¼‰ | å…¨å±€ | åŒå‘ | âŒ ä¸å˜ | è¯­ä¹‰ç©ºé—´å¯¹é½ |
| **ä¸­é—´èåˆ** | ALIGNNå±‚å†… | åŸå­çº§ | å•å‘(æ–‡â†’å›¾) | âŒ ä¸å˜ | æ–‡æœ¬è°ƒåˆ¶å›¾ç¼–ç  |
| **ç»†ç²’åº¦æ³¨æ„åŠ›** | å›¾ç¼–ç å | åŸå­-è¯çº§ | åŒå‘ | âŒ ä¸å˜ | ç²¾ç»†è¯­ä¹‰å¯¹é½ |
| **å…¨å±€æ³¨æ„åŠ›** | æœ€ç»ˆèåˆ | å…¨å±€ | åŒå‘ | âŒ ä¸å˜ | å…¨å±€ä¿¡æ¯èåˆ |

---

## ğŸ”¬ ç‰¹å¾ç©ºé—´å˜åŒ–ç¤ºæ„

### æ— äº¤äº’ï¼ˆåŸºç¡€æ¨¡å‹ï¼‰
```
å›¾ç©ºé—´:     â—  â—  â—  â—  â—
                â†“
æ–‡æœ¬ç©ºé—´:   â—‹  â—‹  â—‹  â—‹  â—‹
           (ä¸¤ä¸ªç‹¬ç«‹ç©ºé—´)
```

### å¯¹æ¯”å­¦ä¹ å
```
å¯¹é½ç©ºé—´:   â—â—‹  â—â—‹  â—â—‹  â—â—‹  â—â—‹
          (å¯¹åº”çš„å›¾-æ–‡æœ¬å¯¹é è¿‘)
```

### ä¸­é—´èåˆå
```
æ–‡æœ¬â†’å›¾:    â—' â—' â—' â—' â—'
            â†‘æ–‡æœ¬å¢å¼ºçš„å›¾ç‰¹å¾
æ–‡æœ¬:       â—‹  â—‹  â—‹  â—‹  â—‹
           (æ–‡æœ¬æœªå˜)
```

### ç»†ç²’åº¦æ³¨æ„åŠ›å
```
äº’ç›¸å¢å¼º:   â—' â—' â—' â—' â—'
            â†•ï¸ åŒå‘å¢å¼º
            â—‹' â—‹' â—‹' â—‹' â—‹'
```

### å…¨å±€æ³¨æ„åŠ›å
```
æœ€ç»ˆèåˆ:   âŠ•  âŠ•  âŠ•  âŠ•  âŠ•
          (å›¾æ–‡æ·±åº¦èåˆçš„ç»Ÿä¸€è¡¨ç¤º)
```

---

## ğŸ’¡ å®é™…åº”ç”¨å»ºè®®

### é€‰æ‹©èåˆç­–ç•¥

1. **åªè¦åŸºç¡€å¯¹é½**:
   - å¯ç”¨: `use_contrastive_loss=True`
   - é€‚åˆ: é¢„è®­ç»ƒã€ç‰¹å¾æå–

2. **éœ€è¦æ–‡æœ¬å¼•å¯¼å›¾ç¼–ç **:
   - å¯ç”¨: `use_middle_fusion=True`
   - é€‚åˆ: æ–‡æœ¬æè¿°å¯¹ç»“æ„ç†è§£å¾ˆé‡è¦çš„ä»»åŠ¡

3. **éœ€è¦å¯è§£é‡Šæ€§**:
   - å¯ç”¨: `use_fine_grained_attention=True`
   - é€‚åˆ: éœ€è¦çŸ¥é“å“ªäº›åŸå­å¯¹åº”å“ªäº›è¯

4. **æœ€å¤§èåˆæ•ˆæœ**:
   - åŒæ—¶å¯ç”¨å¤šä¸ªæœºåˆ¶
   - é€‚åˆ: å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡

---

## ğŸ“ ä»£ç ä½ç½®ç´¢å¼•

- **å¯¹æ¯”å­¦ä¹ **: `alignn.py:69-118`, `1008-1012`
- **ä¸­é—´èåˆ**: `alignn.py:121-218`, `896-899`
- **ç»†ç²’åº¦æ³¨æ„åŠ›**: `alignn.py:352-528`, `905-952`
- **å…¨å±€æ³¨æ„åŠ›**: `alignn.py:221-349`, `960-971`
- **ç‰¹å¾æå–**: `visualize_latent_space.py:57-157`

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2025-11-22
